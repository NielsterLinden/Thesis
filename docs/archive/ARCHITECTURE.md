# System Architecture

This document provides a detailed overview of the thesis-ml system architecture, component responsibilities, and data flow patterns.

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERACTION                        â”‚
â”‚                                                                  â”‚
â”‚  thesis-train             thesis-report          notebooks       â”‚
â”‚      â”‚                         â”‚                     â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚                     â”‚
       â–¼                         â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Layer      â”‚    â”‚  Reports CLI    â”‚   â”‚  Direct Import  â”‚
â”‚                  â”‚    â”‚                 â”‚   â”‚                 â”‚
â”‚  cli/train/      â”‚    â”‚  cli/reports/   â”‚   â”‚  (notebooks)    â”‚
â”‚  - __main__.py   â”‚    â”‚  - __main__.py  â”‚   â”‚                 â”‚
â”‚  - DISPATCH      â”‚    â”‚                 â”‚   â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                      â”‚
         â”‚                       â”‚                      â”‚
         â–¼                       â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CORE EXECUTION LAYER                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Training Loops  â”‚     â”‚   Reports    â”‚    â”‚ Architectures â”‚ â”‚
â”‚  â”‚                 â”‚     â”‚              â”‚    â”‚               â”‚ â”‚
â”‚  â”‚ - autoencoder   â”‚     â”‚ - analyses/  â”‚    â”‚ - autoencoder/â”‚ â”‚
â”‚  â”‚ - gan_ae        â”‚     â”‚ - inference/ â”‚    â”‚ - simple/     â”‚ â”‚
â”‚  â”‚ - diffusion_ae  â”‚     â”‚ - plots/     â”‚    â”‚               â”‚ â”‚
â”‚  â”‚ - simple_mlp    â”‚     â”‚              â”‚    â”‚               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                    â”‚
            â”‚                     â”‚                    â”‚
            â–¼                     â–¼                    â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
   â”‚  Facts System   â”‚   â”‚  Facts Readers  â”‚          â”‚
   â”‚                 â”‚   â”‚                 â”‚          â”‚
   â”‚  - builders     â”‚   â”‚  - load_runs()  â”‚          â”‚
   â”‚  - writers      â”‚â—„â”€â”€â”¤  - discover     â”‚          â”‚
   â”‚                 â”‚   â”‚                 â”‚          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
            â”‚                                          â”‚
            â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Monitoring     â”‚   â”‚      Data       â”‚
   â”‚                 â”‚   â”‚                 â”‚
   â”‚  - orchestrator â”‚   â”‚  - h5_loader    â”‚
   â”‚  - families/    â”‚   â”‚  - synthetic    â”‚
   â”‚                 â”‚   â”‚                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   File System   â”‚
   â”‚                 â”‚
   â”‚  outputs/runs/  â”‚
   â”‚    â”œâ”€ facts/    â”‚
   â”‚    â””â”€ figures/  â”‚
   â”‚                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow: Training Phase

### 1. Initialization

```
User Command
    â”‚
    â–¼
thesis-train [args]
    â”‚
    â–¼
cli/train/__main__.py
    â”‚
    â”œâ”€ Load Hydra config from configs/
    â”œâ”€ Validate legacy keys
    â””â”€ Dispatch to training loop via DISPATCH dict
         â”‚
         â–¼
training_loops/autoencoder.py::train(cfg)
```

### 2. Training Loop Execution

```
train(cfg)
    â”‚
    â”œâ”€ 1. Set random seeds (reproducibility)
    â”‚      â””â”€ utils/seed.py::set_all_seeds()
    â”‚
    â”œâ”€ 2. Load data
    â”‚      â””â”€ data/h5_loader.py::make_dataloaders()
    â”‚
    â”œâ”€ 3. Build model
    â”‚      â””â”€ architectures/autoencoder/base.py::build_from_config()
    â”‚          â”œâ”€ Build encoder (from config)
    â”‚          â”œâ”€ Build bottleneck (VQ/linear/identity)
    â”‚          â””â”€ Build decoder (from config)
    â”‚
    â”œâ”€ 4. Create optimizer
    â”‚      â””â”€ torch.optim.Adam(...)
    â”‚
    â”œâ”€ 5. Setup run directory
    â”‚      â””â”€ Hydra handles this (outputs/runs/run_TIMESTAMP_NAME/)
    â”‚
    â”œâ”€ 6. Emit on_start event
    â”‚      â”œâ”€ facts/builders.py::build_event_payload()
    â”‚      â”œâ”€ facts/writers.py::append_jsonl_event()
    â”‚      â””â”€ monitoring/orchestrator.py::handle_event()
    â”‚
    â”œâ”€ 7. Training epochs
    â”‚      â”‚
    â”‚      FOR each epoch:
    â”‚          â”œâ”€ Run train batches (forward, backward, optimizer step)
    â”‚          â”œâ”€ Run validation batches
    â”‚          â”œâ”€ Compute metrics
    â”‚          â”‚
    â”‚          â”œâ”€ Emit on_epoch_end event
    â”‚          â”‚   â”œâ”€ build_event_payload(histories=...)
    â”‚          â”‚   â”œâ”€ append_jsonl_event()
    â”‚          â”‚   â”œâ”€ append_scalars_csv()  # CSV for easy DataFrame loading
    â”‚          â”‚   â””â”€ handle_event()  # Creates figures if enabled
    â”‚          â”‚
    â”‚          â”œâ”€ Save best checkpoint if val loss improved
    â”‚          â””â”€ Update progress bar
    â”‚
    â”œâ”€ 8. Test evaluation
    â”‚      â””â”€ Run test batches
    â”‚
    â”œâ”€ 9. Emit on_train_end event
    â”‚      â”œâ”€ build_event_payload(total_time_s=...)
    â”‚      â”œâ”€ append_jsonl_event()
    â”‚      â””â”€ handle_event()  # Final plots
    â”‚
    â””â”€ 10. Return results dict
            â””â”€ {"best_val_loss": ..., "test_loss": ...}
```

### 3. Facts Emission

Each training loop emits standardized events:

```python
# Build payload
payload = build_event_payload(
    moment="on_epoch_end",
    run_dir=outdir,
    epoch=ep,
    train_loss=tr["loss"],
    val_loss=va["loss"],
    metrics={"perplex": ...},
    histories={
        "train_loss": [0.5, 0.4, 0.3, ...],
        "val_loss": [0.6, 0.5, 0.4, ...],
        ...
    },
    cfg=cfg,  # For metadata extraction
)

# Write to disk
append_jsonl_event(run_dir, payload)      # events.jsonl
append_scalars_csv(run_dir, epoch=ep, ...) # scalars.csv

# Optionally create plots
handle_event(cfg.logging, families, "on_epoch_end", payload)
```

## ğŸ“Š Data Flow: Reporting Phase

### 1. Report Invocation

```
thesis-report --config-name compare_tokenizers \
    inputs.sweep_dir=outputs/multiruns/exp_TIMESTAMP_NAME

    â”‚
    â–¼
cli/reports/__main__.py
    â”‚
    â”œâ”€ Load report config from configs/report/
    â”œâ”€ Infer report module from config name
    â”‚   â””â”€ reports/analyses/compare_tokenizers.py
    â”‚
    â”œâ”€ Extract data config from first run (for inference)
    â””â”€ Call run_report(cfg)
```

### 2. Report Execution

```
reports/analyses/compare_tokenizers.py::run_report(cfg)
    â”‚
    â”œâ”€ 1. Setup environment
    â”‚      â”œâ”€ Create report directories
    â”‚      â”‚   â”œâ”€ outputs/reports/report_TIMESTAMP_NAME/
    â”‚      â”‚   â”œâ”€   â”œâ”€ training/
    â”‚      â”‚   â””â”€   â””â”€ inference/
    â”‚      â”‚
    â”‚      â””â”€ Discover runs
    â”‚          â””â”€ facts/readers.py::discover_runs(sweep_dir)
    â”‚
    â”œâ”€ 2. Load facts from all runs
    â”‚      â””â”€ facts/readers.py::load_runs()
    â”‚          â”œâ”€ Read .hydra/config.yaml
    â”‚          â”œâ”€ Read facts/events.jsonl
    â”‚          â”œâ”€ Read facts/scalars.csv
    â”‚          â”œâ”€ Extract metadata
    â”‚          â”œâ”€ Compute aggregates
    â”‚          â””â”€ Return (runs_df, per_epoch, order)
    â”‚
    â”œâ”€ 3. Filter runs (optional)
    â”‚      â””â”€ Based on cfg.inputs.select
    â”‚
    â”œâ”€ 4. Save training summary
    â”‚      â”œâ”€ training/summary.csv
    â”‚      â””â”€ training/summary.json
    â”‚
    â”œâ”€ 5. Generate training plots
    â”‚      â”œâ”€ reports/plots/curves.py::plot_loss_vs_time()
    â”‚      â”œâ”€ Custom analysis plots
    â”‚      â””â”€ Save to training/figures/
    â”‚
    â”œâ”€ 6. Run inference (if enabled)
    â”‚      â”œâ”€ Load models from runs
    â”‚      â”œâ”€ reports/inference/forward_pass.py
    â”‚      â”œâ”€ reports/inference/metrics.py
    â”‚      â”œâ”€ reports/inference/anomaly_detection.py
    â”‚      â”œâ”€ Compute AUROC, reconstruction errors
    â”‚      â””â”€ Save to inference/figures/
    â”‚
    â””â”€ 7. Finalize
           â”œâ”€ Create manifest.yaml
           â”œâ”€ Create backlinks to runs
           â””â”€ Log completion
```

## ğŸ—‚ï¸ Component Responsibilities

### `cli/`

**Purpose**: Command-line interface entry points

- `cli/train/`: Training CLI
  - `__main__.py`: Hydra entry point, validation, dispatch
  - `__init__.py`: DISPATCH dictionary mapping loop names to functions

- `cli/reports/`: Reports CLI
  - `__main__.py`: Hydra entry point, report discovery, invocation

**Responsibilities**:
- Parse command-line arguments via Hydra
- Validate configuration
- Dispatch to appropriate training loop or report
- Handle errors gracefully

### `training_loops/`

**Purpose**: Implement training procedures

Each file implements a `train(cfg: DictConfig) -> dict` function:

- `autoencoder.py`: Standard autoencoder training
- `gan_autoencoder.py`: GAN-based autoencoder
- `diffusion_autoencoder.py`: Diffusion-based autoencoder
- `simple_mlp.py`: Simple MLP for testing/debugging

**Responsibilities**:
- Load data via `data/`
- Build model via `architectures/`
- Run training loop
- Emit facts via `facts.writers`
- Optionally create real-time plots via `monitoring.orchestrator`
- Save checkpoints
- Return final metrics

**Key Pattern**:
```python
def train(cfg: DictConfig) -> dict:
    # Setup
    set_all_seeds(cfg.phase1.trainer.seed)
    device = ...

    # Data & model
    train_dl, val_dl, test_dl, meta = make_dataloaders(cfg)
    model = build_from_config(cfg).to(device)
    opt = torch.optim.Adam(...)

    # Training loop
    for epoch in range(cfg.phase1.trainer.epochs):
        # Train, validate, emit facts
        ...

    # Return
    return {"best_val_loss": ..., "test_loss": ...}
```

### `architectures/`

**Purpose**: Model architecture definitions

- `autoencoder/`: Autoencoder components
  - `base.py`: Assembly logic (`build_from_config`)
  - `encoders/`: Encoder modules (MLP, GNN, diffusion, GAN)
  - `decoders/`: Decoder modules (MLP, GNN, diffusion, GAN)
  - `bottlenecks/`: Latent space transformations (VQ, linear, identity)
  - `losses/`: Loss functions (reconstruction, adversarial, diffusion)

- `simple/`: Simple architectures
  - `mlp.py`: Basic MLP builder

**Responsibilities**:
- Define PyTorch nn.Module classes
- Provide builder functions that accept config dicts
- Remain agnostic to training procedure
- Focus on forward pass logic

**Key Pattern**:
```python
def build_encoder(cfg, input_dim, latent_dim):
    """Build encoder from config."""
    return EncoderMLP(
        input_dim=input_dim,
        latent_dim=latent_dim,
        hidden_dims=cfg.phase1.encoder.hidden_dims,
        ...
    )
```

### `facts/`

**Purpose**: Standardized event and metric logging

- `builders.py`: Create event payloads
  - `build_event_payload()`: Constructs standardized dict with metadata

- `writers.py`: Write facts to disk
  - `append_jsonl_event()`: Append to `facts/events.jsonl`
  - `append_scalars_csv()`: Append to `facts/scalars.csv`
  - `ensure_facts_dir()`: Create facts directory

- `readers.py`: Read facts from runs
  - `discover_runs()`: Find run directories
  - `load_runs()`: Load and aggregate facts into DataFrames
  - Helper functions for metadata extraction

**Responsibilities**:
- Define standardized fact schema (schema_version=1)
- Provide consistent API for emitting facts
- Enable efficient reading for reports
- Extract metadata from Hydra configs

**Key Invariant**: Training loops emit, reports consume. No direct coupling.

### `monitoring/`

**Purpose**: Real-time visualization during training

- `orchestrator.py`: Route events to plot families
  - `handle_event()`: Main dispatch function

- `families/`: Plot family implementations
  - `losses.py`: Loss curves
  - `metrics.py`: Metric plots
  - `recon.py`: Reconstruction visualizations
  - `codebook.py`: VQ codebook analysis
  - `latency.py`: Training speed plots
  - `adversarial.py`: GAN-specific plots
  - `diffusion.py`: Diffusion-specific plots

- `io_utils.py`: Figure utilities
  - `ensure_figures_dir()`, `build_filename()`, `save_figure()`

**Responsibilities**:
- Create figures during training (optional, configured via `logging` group)
- Respect `cfg.logging.families` and `cfg.logging.moments` settings
- Save figures to `{run_dir}/figures/`
- Never crash training (fail-safe, log warnings)

**Key Pattern**:
```python
def handle_event(cfg_logging, supported_families, moment, payload):
    families = get_enabled_families(cfg_logging, supported_families, moment)

    for family in families:
        figs = family.handle(moment, payload, cfg_logging)
        for fig in figs:
            save_figure(fig, figures_dir, ...)
```

### `reports/`

**Purpose**: Post-training analysis and inference

- `analyses/`: Report implementations
  - Each file implements `run_report(cfg) -> None`
  - `compare_tokenizers.py`: Compare VQ vs non-VQ
  - `compare_globals_heads.py`: Analyze globals reconstruction

- `inference/`: Inference utilities
  - `forward_pass.py`: Run models on test data
  - `metrics.py`: Compute reconstruction metrics
  - `anomaly_detection.py`: AUROC for anomaly detection
  - `data_corruption.py`: Data corruption strategies

- `plots/`: Report plotting functions
  - `curves.py`: Loss vs time plots
  - `grids.py`: Grid visualizations
  - `scatter.py`: Scatter plots
  - `anomaly.py`: Anomaly detection plots

- `utils/`: Report utilities
  - `io.py`: File I/O helpers
  - `manifest.py`: Manifest generation
  - `backlinks.py`: Create backlinks to runs
  - `inference.py`: Inference orchestration

**Responsibilities**:
- Read facts via `facts.readers`
- Aggregate and compare runs
- Generate comparative plots
- Optionally run inference
- Save outputs to `outputs/reports/report_TIMESTAMP_NAME/`

### `data/`

**Purpose**: Dataset loading and preprocessing

- `h5_loader.py`: Load HDF5 datasets
- `synthetic.py`: Generate synthetic data for testing

**Responsibilities**:
- Return PyTorch DataLoaders
- Return metadata dict (e.g., `{"n_tokens": 100, "cont_dim": 4}`)
- Handle train/val/test splits
- Remain agnostic to model architecture

### `utils/`

**Purpose**: General-purpose utilities

- `seed.py`: `set_all_seeds()` for reproducibility
- `paths.py`: Path management (run IDs, report IDs)
- `training_progress_shower.py`: ASCII progress bars

**Responsibilities**:
- Provide reusable utilities
- No domain-specific logic
- No dependencies on other thesis_ml modules (except minimal imports)

## ğŸ” Key Design Decisions

### 1. Facts-First Architecture

**Decision**: Training loops emit facts; reports consume facts. No direct coupling.

**Rationale**:
- **Reproducibility**: Re-run analysis without re-training
- **Efficiency**: Analysis is cheap; training is expensive
- **Flexibility**: Add new analyses without modifying training code
- **Debugging**: Inspect facts to diagnose issues

**Tradeoff**: Requires consistent fact schema (currently `schema_version=1`)

### 2. Hydra-Driven Configuration

**Decision**: Use Hydra for all configuration, no hardcoded parameters.

**Rationale**:
- **Reproducibility**: `.hydra/config.yaml` captures exact config
- **Flexibility**: Override any parameter from CLI
- **Modularity**: Compose configs from groups
- **Scalability**: Same code, different environments (local/HPC)

**Tradeoff**: Learning curve for Hydra composition

### 3. Separate CLI and Logic

**Decision**: `cli/` contains entry points; logic lives in `training_loops/` and `reports/`.

**Rationale**:
- **Testability**: Import and call `train(cfg)` directly in tests/notebooks
- **Reusability**: Use training loops without CLI
- **Clarity**: Separation of concerns

**Tradeoff**: Slightly more files

### 4. Monitoring is Optional

**Decision**: Training loops emit events; monitoring creates plots if enabled.

**Rationale**:
- **Performance**: Disable plotting on HPC for speed
- **Flexibility**: Different plot policies for different runs
- **Robustness**: Plotting failures don't crash training

**Tradeoff**: Monitoring code must handle missing data gracefully

### 5. Namespace Preservation

**Decision**: Keep `thesis_ml/` as root package, not top-level `train/`, `data/`, etc.

**Rationale**:
- **Avoid Collisions**: Generic names like `train`, `utils` clash with other packages
- **Clean Imports**: `from thesis_ml.facts import ...` is clear
- **pip install**: Works seamlessly with editable installs

**Tradeoff**: Slightly longer import paths

## ğŸ”„ Extension Points

### Adding a New Training Loop

1. Create `training_loops/my_loop.py`
2. Implement `def train(cfg: DictConfig) -> dict`
3. Emit facts via `facts.writers`
4. Register in `cli/train/__init__.py`

### Adding a New Architecture Component

1. Create `architectures/autoencoder/{encoders,decoders,bottlenecks}/my_component.py`
2. Create config in `configs/phase1/{encoder,decoder,latent_space}/my_component.yaml`
3. Update `architectures/autoencoder/base.py` if new type

### Adding a New Report

1. Create `reports/analyses/my_report.py`
2. Implement `def run_report(cfg: DictConfig) -> None`
3. Create config in `configs/report/my_report.yaml`
4. Use `facts.readers.load_runs()` to get data

### Adding a New Plot Family

1. Create `monitoring/families/my_family.py`
2. Implement handler with `handle(moment, payload, cfg) -> list[Figure]`
3. Register in `monitoring/registry.py`

### WandB Integration

**Core code:** `src/thesis_ml/utils/wandb_utils.py` â€” `init_wandb()`, `extract_wandb_config()`, `log_metrics()`, `finish_wandb()`, `log_artifact()`

**Scripts:** `scripts/wandb/` â€” `cleanup_wandb.py`, `migrate_runs_to_wandb.py`, `sync_wandb.sh`, `test_wandb_hpc.py`, `backfill_labels.py`

**Configs:** `configs/logging/` â€” `wandb_online`, `wandb_offline`, `default`

**Auth:** `hpc/stoomboot/.wandb_env` (local and HPC; gitignored). Create with `export WANDB_API_KEY="your_key_here"`.

**SOP for new config keys:**

1. Add key to Hydra config YAML
2. Optionally add curated extraction in `extract_wandb_config()` for clean dashboard UX
3. `raw/*` auto-flatten ensures new keys are never lost
4. Run `python scripts/wandb/backfill_labels.py --dry-run --labels '{"new/key": "default"}'` to stamp old runs with the default value

## ğŸ“ File System Conventions

### Run Directory Structure

```
outputs/runs/run_20251103-140953_experiment_job0/
â”œâ”€â”€ .hydra/
â”‚   â”œâ”€â”€ config.yaml          # Canonical config snapshot
â”‚   â”œâ”€â”€ overrides.yaml       # CLI overrides
â”‚   â””â”€â”€ hydra.yaml           # Hydra runtime config
â”œâ”€â”€ facts/
â”‚   â”œâ”€â”€ events.jsonl         # Lifecycle events (one per line)
â”‚   â””â”€â”€ scalars.csv          # Per-epoch metrics (DataFrame-friendly)
â”œâ”€â”€ figures/                 # Training-time plots (optional)
â”‚   â”œâ”€â”€ losses-on_epoch_end-e019.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ model.pt                 # Symlink/copy of best_val.pt
â”œâ”€â”€ best_val.pt              # Best validation checkpoint
â””â”€â”€ last.pt                  # Final epoch checkpoint
```

### Report Directory Structure

```
outputs/reports/report_20251103-142813_compare_tokenizers/
â”œâ”€â”€ manifest.yaml            # Report metadata
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ summary.csv          # Aggregated metrics across runs
â”‚   â”œâ”€â”€ summary.json         # Metadata, sweep params
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ figure-val_mse_vs_time.png
â”‚       â””â”€â”€ ...
â””â”€â”€ inference/               # Optional
    â”œâ”€â”€ summary.json         # Inference results
    â””â”€â”€ figures/
        â”œâ”€â”€ figure-reconstruction_error_distributions.png
        â””â”€â”€ ...
```

## ğŸš€ Performance Considerations

### Training

- **Disable plots on HPC**: `logging.save_artifacts=false` or `logging.make_plots=false`
- **Use GPU**: Automatically detected via `torch.cuda.is_available()`
- **Batch size**: Tune via config (`data.batch_size`)

### Reports

- **Inference is expensive**: Only enable when needed (`inference.enabled=true`)
- **DataFrame operations**: `facts.readers` returns pandas DataFrames for speed
- **Parallel loading**: Can load runs in parallel (future enhancement)

## ğŸ” Debugging Tips

### Training Issues

1. Check `.hydra/config.yaml` for actual config used
2. Check `facts/events.jsonl` for emitted events
3. Check `facts/scalars.csv` for per-epoch metrics
4. Enable `logging.make_plots=true` to visualize training

### Report Issues

1. Check `manifest.yaml` for report metadata
2. Check `training/summary.csv` for aggregated metrics
3. Verify runs have `on_train_end` event in `facts/events.jsonl`
4. Check logs for warnings about skipped runs

### Import Issues

1. Ensure editable install: `pip install -e .`
2. Check Python path: `echo $PYTHONPATH`
3. Verify package structure: `python -c "import thesis_ml; print(thesis_ml.__file__)"`

## ğŸ¯ Future Enhancements

- **Distributed training**: Add PyTorch DDP support
- **Experiment tracking**: Integrate W&B/MLflow
- **Auto-scaling**: Adjust batch size based on GPU memory
- **Caching**: Cache data loaders for faster iteration
- **Profiling**: Add performance profiling hooks
- **Schema evolution**: Support multiple fact schema versions
