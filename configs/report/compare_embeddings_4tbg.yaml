# Hydra config for report: compare_embeddings_4tbg
#
# Compares classifier performance across different embedding strategies
# in the emb_pe_4tbg sweep:
# - Tokenizer: raw vs identity
# - MET tokens: include_met true vs false
# - Model sizes: s100k vs s500k
#
# Figures are generated by src/thesis_ml/reports/analyses/compare_embeddings_4tbg.py

inputs:
  sweep_dir: null           # Path to experiment directory (exp_*_emb_pe_4tbg)
  run_dirs: []              # Alternative: explicit list of run dirs
  select:
    seed: null              # e.g., [42, 43, 44]

report_name: compare_embeddings_4tbg

outputs:
  report_subdir: report
  fig_format: png
  dpi: 150
  which_figures:
    # Training comparison plots
    - all_val_curves
    - all_train_curves
    - val_loss_by_embedding
    - val_auroc_by_embedding
    # Inference plots (requires inference.enabled=true)
    - roc_curves
    - metrics_comparison
    - confusion_matrices
    - score_distributions
    # Grouped by embedding_type
    - roc_curves_by_embedding
    - auroc_bar_by_embedding

thresholds:
  val_acc: 0.8
  comparison: ge
  split: val

summary_schema_version: 1

inference:
  enabled: true             # Enable to run inference during report generation
  persist_raw_scores: false # If true, save per-event scores
  dataset_split: test       # Which split to evaluate on
  autocast: true
  batch_size: 512
  seed: 42
  n_points_roc: 250
  max_samples: null         # Limit number of samples for testing (null = use all data)

env:
  output_root: null         # Set explicitly when running (e.g., /data/atlas/users/nterlind/outputs)
