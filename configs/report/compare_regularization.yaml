# Hydra config for report: compare_regularization
#
# Compares classifier performance across different regularization settings:
# - dropout
# - weight_decay
# - learning rate
# - label_smoothing

inputs:
  sweep_dir: null           # Path to experiment directory
  run_dirs: []              # Alternative: explicit list of run dirs
  select:
    dropout: null           # e.g., [0.0, 0.1, 0.2, 0.3]
    weight_decay: null      # e.g., [0.0, 0.0001, 0.001]
    lr: null                # e.g., [0.0001, 0.0003, 0.001]
    seed: null              # e.g., [42, 43, 44]

report_name: compare_regularization

outputs:
  report_subdir: report
  fig_format: png
  dpi: 150
  which_figures:
    # Training comparison plots
    - all_val_curves
    - all_train_curves
    # Grouped by dropout
    - val_loss_by_dropout
    - val_auroc_by_dropout
    # Grouped by weight_decay
    - val_loss_by_weight_decay
    - val_auroc_by_weight_decay
    # Grouped by learning rate
    - val_loss_by_lr
    - val_auroc_by_lr
    # Inference plots (requires inference.enabled=true)
    - roc_curves
    - metrics_comparison
    - confusion_matrices
    - score_distributions
    # Grouped ROC curves
    - roc_curves_by_dropout
    - roc_curves_by_weight_decay
    # AUROC bar charts
    - auroc_bar_by_dropout
    - auroc_bar_by_weight_decay
    - auroc_bar_by_lr
    # Grid heatmaps for parameter interactions
    - grid_auroc_dropout_vs_weight_decay
    - grid_auroc_dropout_vs_lr

thresholds:
  val_acc: 0.8
  comparison: ge
  split: val

summary_schema_version: 1

inference:
  enabled: true             # Enable to run inference during report generation
  persist_raw_scores: false # If true, save per-event scores
  dataset_split: test       # Which split to evaluate on
  autocast: true
  batch_size: 512
  seed: 42
  n_points_roc: 250
  max_samples: null         # Limit number of samples for testing (null = use all data)

env:
  output_root: null         # Auto-inferred from sweep_dir path, or set explicitly
