# @package _global_
# Experiment to compare positional encoding strategies across model sizes
#
# Sweep dimensions:
#   - 4 positional encodings: none, sinusoidal, learned, rotary
#   - 3 model sizes: small (1/4), medium (1/2), base (full)
#
# Total runs: 4 Ã— 3 = 12 jobs

# Metadata schema - explicit goal for reliable W&B slicing
meta:
  goal: "classification"

experiment:
  name: "compare_positional_encodings"

hydra:
  mode: MULTIRUN
  job:
    chdir: true  # Must be true for Hydra to create .hydra/ directory
    name: ${experiment.name}
  # Override run.dir so each job writes directly to outputs/runs/ (not nested under multirun)
  run:
    dir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweep:
    dir: ${env.output_root}/multiruns/exp_${now:%Y%m%d-%H%M%S}_${experiment.name}
    # Set subdir to absolute path matching run.dir so Hydra changes to run.dir instead of creating numbered subdirs
    # Using the same template ensures both resolve to the same path
    subdir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweeper:
    params:
      # Positional encoding strategies (comma-separated for basic sweeper)
      classifier.model.positional: none,sinusoidal,learned,rotary
      # Model sizes: small (~1/4), medium (~1/2), base (full)
      +classifier/model_size: small,medium,base
      # Training settings
      classifier.trainer.epochs: 100
