# @package _global_
# PID Embedding Deep Dive — Standard Training
#
# Investigate how Particle ID embedding mode and dimensionality affect
# classification performance and embedding geometry.
#
# Sweep axes:
#   - pid_mode:     learned, one_hot, fixed_random
#   - id_embed_dim: 8, 16, 32
#
# Note: one_hot forces id_embed_dim = num_types (=8), so dim overrides
#       are ignored for that mode. This produces 7 unique configs:
#       learned×{8,16,32} + one_hot×{8} + fixed_random×{8,16,32}
#
# Total runs: 7 (after dedup of one_hot @ dim>8)

experiment:
  name: "pid_deepdive_standard"

hydra:
  mode: MULTIRUN
  job:
    chdir: true
    name: ${experiment.name}
  run:
    dir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweep:
    dir: ${env.output_root}/multiruns/exp_${now:%Y%m%d-%H%M%S}_${experiment.name}
    subdir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweeper:
    params:
      # PID embedding mode × dimension
      classifier.model.tokenizer.pid_mode: learned,one_hot,fixed_random
      classifier.model.tokenizer.id_embed_dim: 8,16,32
      # Training settings
      classifier.trainer.epochs: 100
      classifier.trainer.log_pid_embeddings: true

# Metadata schema
meta:
  goal: "classification"

# Force identity tokenizer (PID experiment requires it)
classifier:
  model:
    tokenizer:
      name: identity

# Signal vs background (same as other 4t experiments)
data:
  classifier:
    signal_vs_background:
      signal: 1
      background: [2, 3, 4, 5]
