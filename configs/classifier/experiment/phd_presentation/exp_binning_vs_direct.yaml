# @package _global_
# PhD Presentation - Binning vs Direct Embedding Experiment
#
# Sweep: 3 pooling × 2 tokenization (direct vs binned) × 2 MET × 2 vect = 24 models
# Direct: identity tokenizer (cont + ID embed); Binned: Ambre-style 5-bin integer tokens
#
# Fixed: ~400k params (s400k), sinusoidal PE, normformer, no early stopping, 50 epochs

meta:
  goal: "classification"

experiment:
  name: "exp_binning_vs_direct"

hydra:
  mode: MULTIRUN
  job:
    chdir: true
    name: ${experiment.name}
  run:
    dir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweep:
    dir: ${env.output_root}/multiruns/exp_${now:%Y%m%d-%H%M%S}_${experiment.name}
    subdir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweeper:
    params:
      # Pooling: cls, mean, max
      classifier.model.pooling: cls,mean,max
      # Tokenization: direct (raw data) vs binned (our 5-bin dataset)
      +data: h5_tokens,h5_tokens_binned
      # MET: with or without MET + MET phi
      classifier.globals.include_met: true,false
      # Vector type: 4-vect (Pt,eta,phi,PID) vs 5-vect (+E)
      data.cont_features: "[1,2,3],[0,1,2,3]"
      # Model: ~400k params
      +classifier/model_size: s400k
      # Fixed architecture
      classifier.model.positional: sinusoidal
      classifier.model.norm.policy: normformer
      classifier.model.tokenizer.name: identity
      classifier.model.tokenizer.id_embed_dim: 8
      classifier.trainer.epochs: 50

# Signal vs background (4t vs ttH+ttW+ttWW+ttZ)
data:
  classifier:
    signal_vs_background:
      signal: 1
      background: [2, 3, 4, 5]
