# @package _global_
# PhD Presentation - Binning vs Direct Embedding Experiment
#
# Sweep: 3 pooling × 3 tokenization (direct, binned, VQ-VAE) × 2 MET × 2 vect = 36 models
# Direct: identity tokenizer; Binned: Ambre-style 5-bin; VQ-VAE: pretrained encoder+VQ
#
# Fixed: ~400k params (s400k), sinusoidal PE, normformer, no early stopping, 50 epochs
#
# VQ-VAE: Pre-train first with loop=ae phase1/latent_space=vq, then copy best_val.pt
#   to env.vq_checkpoint_path (e.g. /data/atlas/users/nterlind/checkpoints/vq_4tops_best.pt)

meta:
  goal: "classification"

experiment:
  name: "exp_binning_vs_direct"

hydra:
  mode: MULTIRUN
  job:
    chdir: true
    name: ${experiment.name}
  run:
    dir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweep:
    dir: ${env.output_root}/multiruns/exp_${now:%Y%m%d-%H%M%S}_${experiment.name}
    subdir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweeper:
    params:
      # Pooling: cls, mean, max
      classifier.model.pooling: cls,mean,max
      # Tokenization: direct, binned, or VQ-VAE (pretrained)
      +tokenization: direct,binned,vq
      # MET: with or without MET + MET phi
      classifier.globals.include_met: true,false
      # Vector type: 4-vect (Pt,eta,phi,PID) vs 5-vect (+E)
      data.cont_features: "[1,2,3],[0,1,2,3]"
      # Model: ~400k params
      +classifier/model_size: s400k
      # Fixed architecture
      classifier.model.positional: sinusoidal
      classifier.model.norm.policy: normformer
      classifier.model.tokenizer.id_embed_dim: 8
      classifier.trainer.epochs: 50

# Signal vs background (4t vs ttH+ttW+ttWW+ttZ)
data:
  classifier:
    signal_vs_background:
      signal: 1
      background: [2, 3, 4, 5]
