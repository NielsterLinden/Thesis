# @package _global_
# Embedding & Positional Encoding Study - 4t vs Background
#
# Grid: 2 tokenizers × 2 MET settings × 6 PE variants × 2 model sizes = 48 runs
#
# Axes:
#   1. Tokenizer: raw (4 continuous) vs identity (4 continuous + ID embedding)
#   2. MET tokens: include_met true vs false
#   3. Positional encoding: 6 variants via +classifier/pos_variant
#   4. Model size: s100k vs s500k
#
# Task: 4t (label 1) vs all background (labels 2,3,4,5)

meta:
  goal: "classification"

experiment:
  name: "emb_pe_4tbg"

hydra:
  mode: MULTIRUN
  job:
    chdir: true
    name: ${experiment.name}
  run:
    dir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweep:
    dir: ${env.output_root}/multiruns/exp_${now:%Y%m%d-%H%M%S}_${experiment.name}
    subdir: ${env.output_root}/runs/run_${now:%Y%m%d-%H%M%S}_${experiment.name}_job${zpad:${hydra.job.num}}
  sweeper:
    params:
      # Model sizes (2)
      +classifier/model_size: s100k,s500k

      # Tokenizer (2): raw vs identity
      classifier.model.tokenizer.name: raw,identity
      classifier.model.tokenizer.id_embed_dim: 8

      # MET tokens (2): on vs off
      classifier.globals.include_met: true,false

      # Positional encoding (6 variants via config group)
      +classifier/pos_variant: none,sinusoidal_full,sinusoidal_cont,learned_full,learned_cont,rotary

      # Pooling and normalization (fixed)
      classifier.model.pooling: cls
      classifier.model.norm.policy: post

      # Training (default 50 epochs, override on CLI for dry runs)
      classifier.trainer.epochs: 50

# Task: signal vs background
data:
  classifier:
    signal_vs_background:
      signal: 1
      background: [2, 3, 4, 5]
    selected_labels: null
