# @package _global_
# Transformer classifier model configuration

classifier:
  model:
    # Architecture dimensions
    dim: 256              # Model dimension (hidden size)
    depth: 6              # Number of transformer encoder layers
    heads: 8              # Number of attention heads
    mlp_dim: 1024         # MLP hidden dimension (typically 4x dim)
    dropout: 0.1          # Dropout rate

    # Normalization policy
    norm:
      policy: pre         # "pre", "post", or "normformer" normalization

    # Positional encoding
    positional: sinusoidal  # "none", "sinusoidal", "learned", or "rotary"

    # Positional encoding space (where PE is applied)
    positional_space: model  # "model" (after projection, old behavior) or "token" (before projection, for selective PE)

    # Positional encoding dimension mask (only used when positional_space: "token")
    # null: apply PE to all dimensions
    # List of ints: explicit dimension indices (e.g., [0, 1, 2, 3])
    # List of strings: semantic names (e.g., ["id"], ["E", "Pt"], ["continuous"])
    #   Available names: "E", "Pt", "eta", "phi", "continuous", "id" (only for IdentityTokenizer)
    positional_dim_mask: null

    # Rotary positional encoding options (only used when positional: rotary)
    rotary:
      base: 10000.0       # Base for computing inverse frequencies

    # Causal attention (encoder: position i cannot attend to j > i)
    causal_attention: false

    # Pooling strategy
    pooling: cls          # "cls" (first token) or "mean" (average pooling)

    # Tokenizer (optional pre-embedding transformation)
    tokenizer:
      name: raw           # "identity", "raw", "binned", or "pretrained"
      id_embed_dim: 8     # Dimension for ID embedding in identity tokenizer (only for "identity")
      # For "binned": vocab_size will be inferred from data (typically 886)
      # For "pretrained": checkpoint_path and model_type must be specified
