{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup (Phase 1 inference)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from thesis_ml.data.h5_loader import H5TokenDataset\n",
    "from thesis_ml.utils.phase1_infer import load_model_from_run\n",
    "\n",
    "# Change this to the run you want to inspect\n",
    "run_dir = r\"C:\\Users\\niels\\Projects\\Thesis-Code\\Code\\Niels_repo\\outputs\\20251021-164951\"\n",
    "\n",
    "# Load composed cfg and assembled Phase 1 model from the run directory\n",
    "cfg, model, device = load_model_from_run(run_dir)\n",
    "print(\"device:\", device)\n",
    "print(\"loaded targets:\", cfg.phase1.encoder._target_, cfg.phase1.tokenizer._target_, cfg.phase1.decoder._target_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset (same normalization as training)\n",
    "# Build dataset from composed cfg saved in the run dir\n",
    "# (H5TokenDataset reads cfg.data.path and computes mu/sd on train split)\n",
    "ds = H5TokenDataset(cfg)\n",
    "val_ds = ds.get_split(\"val\")\n",
    "test_ds = ds.get_split(\"test\")\n",
    "\n",
    "print(\"num val:\", len(val_ds), \"num test:\", len(test_ds))\n",
    "print(\"n_tokens (T):\", int(cfg.data.n_tokens))\n",
    "print(\"cont_dim:\", 4, \"globals:\", int(cfg.data.globals.size), \"num_types:\", ds.num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c61515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model is already loaded; confirm and print basic info\n",
    "print(model.__class__.__name__)\n",
    "print(sum(p.numel() for p in model.parameters()), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper for reconstruction using unified Phase 1 model\n",
    "\n",
    "@torch.no_grad()\n",
    "def reconstruct_event(tokens_cont, tokens_id, gvec):\n",
    "    # inputs are single-event tensors: [T,4], [T], [2]\n",
    "    tc = tokens_cont.unsqueeze(0).to(device)  # [1,T,4]\n",
    "    ti = tokens_id.unsqueeze(0).to(device)    # [1,T]\n",
    "    gv = gvec.unsqueeze(0).to(device)         # [1,2]\n",
    "\n",
    "    out = model(tc, ti, gv)\n",
    "    x_hat = out[\"x_hat\"]                     # [1,T,4] (normalized)\n",
    "    perplex = float(out.get(\"aux\", {}).get(\"perplex\", 0.0))\n",
    "\n",
    "    # De-normalize to original scale with train mu/sd\n",
    "    mu = ds.mu[0,0].to(device)  # [4]\n",
    "    sd = ds.sd[0,0].to(device)  # [4]\n",
    "    tc_denorm     = tc[0] * sd + mu          # [T,4]\n",
    "    x_hat_denorm  = x_hat[0] * sd + mu       # [T,4]\n",
    "\n",
    "    return {\n",
    "        \"x_hat_norm\": x_hat[0].cpu(),        # [T,4]\n",
    "        \"x_hat\": x_hat_denorm.cpu(),         # [T,4] de-normalized\n",
    "        \"orig_norm\": tc[0].cpu(),            # [T,4]\n",
    "        \"orig\": tc_denorm.cpu(),             # [T,4] de-normalized\n",
    "        \"perplexity\": perplex,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Pick an event and a token index, then visualize\n",
    "event_idx = 0     # change this to inspect a different event from the test set\n",
    "token_idx = 15     # 0..T-1 (e.g., 0..17)\n",
    "\n",
    "tokens_cont, tokens_id, gvec = test_ds[event_idx]\n",
    "out = reconstruct_event(tokens_cont, tokens_id, gvec)\n",
    "\n",
    "orig = out[\"orig\"][token_idx]     # [4]\n",
    "recon = out[\"x_hat\"][token_idx]   # [4]\n",
    "\n",
    "print(f\"Event {event_idx}, Token {token_idx}\")\n",
    "print(\"Perplexity (event-level):\", out[\"perplexity\"])\n",
    "print(\"Original (de-normalized):\", orig.numpy())\n",
    "print(\"Reconstruction (de-normalized):\", recon.numpy())\n",
    "\n",
    "# Simple bar comparison\n",
    "feature_names = [\"f1\",\"f2\",\"f3\",\"f4\"]  # replace with actual names if you have them\n",
    "x = range(4)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar([i-0.2 for i in x], orig,  width=0.4, label=\"orig\")\n",
    "plt.bar([i+0.2 for i in x], recon, width=0.4, label=\"recon\")\n",
    "plt.xticks(x, feature_names)\n",
    "plt.title(f\"Event {event_idx}, Token {token_idx}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf119bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Cell 6 (optional): Inspect all tokens for an event at once\n",
    "event_idx = 0\n",
    "tokens_cont, tokens_id, gvec = test_ds[event_idx]\n",
    "out = reconstruct_event(tokens_cont, tokens_id, gvec)\n",
    "\n",
    "# L2 error per token (on de-normalized features)\n",
    "\n",
    "\n",
    "per_token_mse = ((out[\"orig\"] - out[\"x_hat\"])**2).mean(dim=1)  # [T]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(per_token_mse.numpy(), marker=\"o\")\n",
    "plt.xlabel(\"token index\")\n",
    "plt.ylabel(\"MSE (denorm)\")\n",
    "plt.title(f\"Event {event_idx} - token-wise reconstruction error\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47310dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (optional, requires ipywidgets): interactive browsing\n",
    "# pip install ipywidgets if needed; in JupyterLab enable the extension\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import clear_output, display\n",
    "\n",
    "    T = int(cfg.data.n_tokens)\n",
    "    def show(event_idx=0, token_idx=0):\n",
    "        tokens_cont, tokens_id, gvec = test_ds[event_idx]\n",
    "        out = reconstruct_event(tokens_cont, tokens_id, gvec)\n",
    "        orig = out[\"orig\"][token_idx]\n",
    "        recon = out[\"x_hat\"][token_idx]\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Event {event_idx}, Token {token_idx} | Perplexity: {out['perplexity']:.3f}\")\n",
    "        x = range(4)\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.bar([i-0.2 for i in x], orig,  width=0.4, label=\"orig\")\n",
    "        plt.bar([i+0.2 for i in x], recon, width=0.4, label=\"recon\")\n",
    "        plt.xticks(x, [\"f1\",\"f2\",\"f3\",\"f4\"])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    w_event = widgets.IntSlider(value=0, min=0, max=len(test_ds)-1, step=1, description=\"event\")\n",
    "    w_token = widgets.IntSlider(value=0, min=0, max=T-1, step=1, description=\"token\")\n",
    "\n",
    "    ui = widgets.VBox([w_event, w_token])\n",
    "    out_area = widgets.interactive_output(show, {\"event_idx\": w_event, \"token_idx\": w_token})\n",
    "    display(ui, out_area)\n",
    "except Exception as e:\n",
    "    print(\"ipywidgets not available:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
