{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b743d78",
   "metadata": {},
   "source": [
    "# Exploring Positional Encoding Effects in Transformer Classifiers\n",
    "\n",
    "This notebook investigates how different positional encoding strategies affect transformer representations for the 4t vs background classification task.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll analyze four trained models with different positional encodings:\n",
    "1. **None** - No positional information (permutation invariant)\n",
    "2. **Sinusoidal** - Fixed sin/cos patterns from \"Attention is All You Need\"\n",
    "3. **Learned** - Trainable positional embeddings\n",
    "4. **Rotary (RoPE)** - Rotary position embeddings applied in attention\n",
    "\n",
    "## Key Questions\n",
    "- How do positional encodings look in model dimensions?\n",
    "- How do they affect representations layer-by-layer through the transformer?\n",
    "- Why does positional encoding improve performance?\n",
    "- What's the difference between sinusoidal, learned, and rotary approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(r\"C:\\Users\\niels\\Projects\\Thesis-Code\\Code\\Niels_repo\")\n",
    "sys.path.insert(0, str(project_root / \"src\"))  # noqa: E402\n",
    "\n",
    "from thesis_ml.data.h5_loader import make_classification_dataloaders  # noqa: E402\n",
    "\n",
    "# Plotting setup\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf515e",
   "metadata": {},
   "source": [
    "## 1. Load All Four Models\n",
    "\n",
    "Load the trained models with different positional encodings from the experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "models_root = Path(r\"C:\\Users\\niels\\Projects\\Thesis-Code\\Saved models\\20251127-162910_4t_vs_background_postional\")\n",
    "data_path = Path(r\"C:\\Users\\niels\\Projects\\Thesis-Code\\Data\\4tops_splitted.h5\")\n",
    "\n",
    "# Load all four models\n",
    "models_info = {}\n",
    "run_ids = [\n",
    "    \"run_20251127-162910_4t_vs_background_positional_job0\",\n",
    "    \"run_20251127-162910_4t_vs_background_positional_job1\",\n",
    "    \"run_20251127-162910_4t_vs_background_positional_job2\",\n",
    "    \"run_20251127-162910_4t_vs_background_positional_job3\",\n",
    "]\n",
    "\n",
    "# Import additional tools for manual loading\n",
    "from thesis_ml.architectures.transformer_classifier.base import build_from_config as build_classifier  # noqa: E402\n",
    "from thesis_ml.training_loops.transformer_classifier import _gather_meta  # noqa: E402\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Loading models...\")\n",
    "for run_id in run_ids:\n",
    "    try:\n",
    "        # Direct path construction\n",
    "        run_dir = models_root / run_id\n",
    "        cfg_path = run_dir / \".hydra\" / \"config.yaml\"\n",
    "        \n",
    "        if not cfg_path.exists():\n",
    "            print(f\"‚úó Config not found: {cfg_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load config\n",
    "        cfg = OmegaConf.load(str(cfg_path))\n",
    "        \n",
    "        # Override data path\n",
    "        cfg.data.path = str(data_path)\n",
    "        \n",
    "        # Populate meta if missing\n",
    "        if not hasattr(cfg, \"meta\") or cfg.meta is None:\n",
    "            train_dl_temp, val_dl_temp, test_dl_temp, meta = make_classification_dataloaders(cfg)\n",
    "            _gather_meta(cfg, meta)\n",
    "        \n",
    "        # Build model\n",
    "        model = build_classifier(cfg, cfg.meta).to(device)\n",
    "        \n",
    "        # Load weights\n",
    "        best_val_path = run_dir / \"best_val.pt\"\n",
    "        model_pt_path = run_dir / \"model.pt\"\n",
    "        \n",
    "        if best_val_path.exists():\n",
    "            weights_path = best_val_path\n",
    "        elif model_pt_path.exists():\n",
    "            weights_path = model_pt_path\n",
    "        else:\n",
    "            print(f\"‚úó No weights found in {run_dir}\")\n",
    "            continue\n",
    "        \n",
    "        checkpoint = torch.load(str(weights_path), map_location=device, weights_only=False)\n",
    "        state_dict = checkpoint[\"model_state_dict\"] if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint else checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        pos_enc_type = cfg.classifier.model.get(\"positional\", \"unknown\")\n",
    "        models_info[pos_enc_type] = {\n",
    "            \"run_id\": run_id,\n",
    "            \"cfg\": cfg,\n",
    "            \"model\": model,\n",
    "            \"device\": device,\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úì Loaded {run_id}\")\n",
    "        print(f\"  Positional encoding: {pos_enc_type}\")\n",
    "        print(f\"  Model dim: {cfg.classifier.model.dim}\")\n",
    "        print(f\"  Layers: {cfg.classifier.model.depth}\")\n",
    "        print(f\"  Heads: {cfg.classifier.model.heads}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to load {run_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(models_info)} models:\")\n",
    "print(f\"Positional encoding types: {list(models_info.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa119f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (use config from first model)\n",
    "sample_cfg = next(iter(models_info.values()))[\"cfg\"]\n",
    "\n",
    "# Override data path to use our specified location\n",
    "sample_cfg.data.path = str(data_path)\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "train_dl, val_dl, test_dl, meta = make_classification_dataloaders(sample_cfg)\n",
    "\n",
    "print(\"‚úì Dataset loaded\")\n",
    "print(f\"  Path: {data_path}\")\n",
    "print(f\"  n_tokens: {meta['n_tokens']}\")\n",
    "print(f\"  n_classes: {meta['n_classes']}\")\n",
    "print(f\"  has_globals: {meta['has_globals']}\")\n",
    "print(f\"  Train batches: {len(train_dl)}\")\n",
    "print(f\"  Val batches: {len(val_dl)}\")\n",
    "print(f\"  Test batches: {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888a26b",
   "metadata": {},
   "source": [
    "## 2. Visualize Raw Positional Encoding Patterns\n",
    "\n",
    "Extract and visualize the positional encoding matrices before they're added to embeddings.\n",
    "\n",
    "**Note:** Rotary encoding doesn't have an additive positional encoding matrix - it applies rotations inside the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positional encoding matrices\n",
    "pe_matrices = {}\n",
    "\n",
    "for pe_type, info in models_info.items():\n",
    "    model = info[\"model\"]\n",
    "    \n",
    "    if pe_type == \"none\":\n",
    "        # No positional encoding\n",
    "        pe_matrices[pe_type] = None\n",
    "        print(f\"{pe_type}: No positional encoding (identity)\")\n",
    "        \n",
    "    elif pe_type == \"rotary\":\n",
    "        # Rotary is applied in attention, not additive\n",
    "        # Extract the cached sin/cos patterns after they're computed\n",
    "        rotary_emb = model.encoder.rotary_emb\n",
    "        if rotary_emb is not None:\n",
    "            print(f\"{pe_type}: Applied in attention mechanism (not additive)\")\n",
    "            print(f\"  head_dim: {rotary_emb.head_dim}\")\n",
    "            print(f\"  base: {rotary_emb.base}\")\n",
    "            # We'll visualize this differently later\n",
    "            pe_matrices[pe_type] = None\n",
    "        else:\n",
    "            print(f\"{pe_type}: No rotary embedding found\")\n",
    "            pe_matrices[pe_type] = None\n",
    "            \n",
    "    else:\n",
    "        # Sinusoidal or Learned\n",
    "        pos_enc = model.pos_enc\n",
    "        if pos_enc is not None and hasattr(pos_enc, 'pe'):\n",
    "            pe_matrix = pos_enc.pe.detach().cpu().numpy()\n",
    "            pe_matrices[pe_type] = pe_matrix\n",
    "            print(f\"{pe_type}: Shape {pe_matrix.shape} (tokens √ó model_dim)\")\n",
    "        else:\n",
    "            pe_matrices[pe_type] = None\n",
    "            print(f\"{pe_type}: No positional encoding matrix found\")\n",
    "\n",
    "print(\"\\n‚úì Extracted positional encoding patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encoding matrices as heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plot_types = ['sinusoidal', 'learned']\n",
    "for idx, pe_type in enumerate(plot_types):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if pe_type in pe_matrices and pe_matrices[pe_type] is not None:\n",
    "        pe_matrix = pe_matrices[pe_type]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(pe_matrix, aspect='auto', cmap='RdBu_r', \n",
    "                       vmin=-np.abs(pe_matrix).max(), vmax=np.abs(pe_matrix).max())\n",
    "        ax.set_xlabel('Model Dimension', fontsize=12)\n",
    "        ax.set_ylabel('Token Position', fontsize=12)\n",
    "        ax.set_title(f'{pe_type.capitalize()} Positional Encoding\\n({pe_matrix.shape[0]} tokens √ó {pe_matrix.shape[1]} dims)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.colorbar(im, ax=ax, label='Encoding Value')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{pe_type.capitalize()}\\nNot available', \n",
    "                ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Heatmap shows how each position is encoded across all model dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-token patterns: show how each token's encoding varies across dimensions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "plot_types = ['sinusoidal', 'learned']\n",
    "for idx, pe_type in enumerate(plot_types):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if pe_type in pe_matrices and pe_matrices[pe_type] is not None:\n",
    "        pe_matrix = pe_matrices[pe_type]\n",
    "        \n",
    "        # Plot first 5 tokens to show pattern\n",
    "        tokens_to_plot = min(5, pe_matrix.shape[0])\n",
    "        for token_idx in range(tokens_to_plot):\n",
    "            ax.plot(pe_matrix[token_idx, :], label=f'Token {token_idx}', alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Model Dimension', fontsize=12)\n",
    "        ax.set_ylabel('Encoding Value', fontsize=12)\n",
    "        ax.set_title(f'{pe_type.capitalize()} - Encoding Pattern Across Dimensions', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{pe_type.capitalize()}\\nNot available', \n",
    "                ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Sinusoidal: Smooth wave patterns with different frequencies per dimension\")\n",
    "print(\"- Learned: Random-initialized patterns that were optimized during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Rotary: visualize the sin/cos patterns\n",
    "# Need to trigger a forward pass first to populate the cache\n",
    "\n",
    "print(\"Visualizing Rotary (RoPE) sin/cos patterns...\")\n",
    "if 'rotary' in models_info:\n",
    "    model = models_info['rotary']['model']\n",
    "    rotary_emb = model.encoder.rotary_emb\n",
    "    \n",
    "    if rotary_emb is not None:\n",
    "        # Get a sample batch to trigger forward pass\n",
    "        sample_batch = next(iter(val_dl))\n",
    "        device = models_info['rotary']['device']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forward pass to populate rotary cache\n",
    "            if len(sample_batch) == 4:  # (tokens_cont, tokens_id, labels, mask)\n",
    "                tokens_cont, tokens_id, labels, mask = sample_batch\n",
    "                tokens_cont = tokens_cont.to(device)\n",
    "                tokens_id = tokens_id.to(device)\n",
    "                mask = mask.to(device)\n",
    "                _ = model(tokens_cont, tokens_id, mask=mask)\n",
    "            elif len(sample_batch) == 5:  # (tokens_cont, tokens_id, globals, mask, labels)\n",
    "                tokens_cont, tokens_id, globals, mask, labels = sample_batch\n",
    "                tokens_cont = tokens_cont.to(device)\n",
    "                tokens_id = tokens_id.to(device)\n",
    "                mask = mask.to(device)\n",
    "                _ = model(tokens_cont, tokens_id, mask=mask)\n",
    "            else:  # binned format\n",
    "                tokens, labels, mask = sample_batch\n",
    "                tokens = tokens.to(device)\n",
    "                mask = mask.to(device)\n",
    "                _ = model(tokens, mask=mask)\n",
    "        \n",
    "        # Extract cached sin/cos\n",
    "        if hasattr(rotary_emb, '_cos_cached') and rotary_emb._cos_cached is not None:\n",
    "            cos_cache = rotary_emb._cos_cached.squeeze().detach().cpu().numpy()  # [seq_len, head_dim]\n",
    "            sin_cache = rotary_emb._sin_cached.squeeze().detach().cpu().numpy()\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Plot cos\n",
    "            im0 = axes[0].imshow(cos_cache, aspect='auto', cmap='RdBu_r',\n",
    "                               vmin=-1, vmax=1)\n",
    "            axes[0].set_xlabel('Head Dimension', fontsize=12)\n",
    "            axes[0].set_ylabel('Sequence Position', fontsize=12)\n",
    "            axes[0].set_title(f'Rotary Cosine Pattern\\n({cos_cache.shape[0]} positions √ó {cos_cache.shape[1]} head_dims)', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "            plt.colorbar(im0, ax=axes[0], label='Cosine Value')\n",
    "            \n",
    "            # Plot sin\n",
    "            im1 = axes[1].imshow(sin_cache, aspect='auto', cmap='RdBu_r',\n",
    "                               vmin=-1, vmax=1)\n",
    "            axes[1].set_xlabel('Head Dimension', fontsize=12)\n",
    "            axes[1].set_ylabel('Sequence Position', fontsize=12)\n",
    "            axes[1].set_title(f'Rotary Sine Pattern\\n({sin_cache.shape[0]} positions √ó {sin_cache.shape[1]} head_dims)', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "            plt.colorbar(im1, ax=axes[1], label='Sine Value')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"‚úì Rotary patterns visualized\")\n",
    "            print(\"  These are applied to Q and K in attention, not added to embeddings\")\n",
    "        else:\n",
    "            print(\"  No rotary cache populated (cache may not exist or forward pass didn't trigger it)\")\n",
    "else:\n",
    "    print(\"  No rotary model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030c190",
   "metadata": {},
   "source": [
    "## 3. Extract Layer-by-Layer Representations\n",
    "\n",
    "Use forward hooks to capture intermediate representations at each stage:\n",
    "1. After embedding (before positional encoding)\n",
    "2. After positional encoding is added\n",
    "3. After each transformer encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook storage\n",
    "class ActivationCapture:\n",
    "    \"\"\"Capture intermediate activations during forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "    \n",
    "    def register_hook(self, module, name):\n",
    "        \"\"\"Register a forward hook on a module.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Handle different output types\n",
    "            if isinstance(output, tuple):\n",
    "                # Some modules return (output, attention_weights)\n",
    "                self.activations[name] = output[0].detach().cpu()\n",
    "            else:\n",
    "                self.activations[name] = output.detach().cpu()\n",
    "        \n",
    "        handle = module.register_forward_hook(hook)\n",
    "        self.hooks.append(handle)\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for handle in self.hooks:\n",
    "            handle.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "print(\"‚úì ActivationCapture class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract representations from all models\n",
    "def extract_layer_representations(models_info, data_loader, num_batches=1):\n",
    "    \"\"\"Extract layer-by-layer representations from all models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models_info : dict\n",
    "        Dictionary mapping PE type to model info\n",
    "    data_loader : DataLoader\n",
    "        DataLoader to get samples from\n",
    "    num_batches : int\n",
    "        Number of batches to process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Nested dict: {pe_type: {layer_name: tensor [B, T, D]}}\n",
    "    \"\"\"\n",
    "    all_representations = {}\n",
    "    all_labels = {}\n",
    "    \n",
    "    for pe_type, info in models_info.items():\n",
    "        model = info[\"model\"]\n",
    "        device = info[\"device\"]\n",
    "        model.eval()\n",
    "        \n",
    "        # Create activation capture\n",
    "        capture = ActivationCapture()\n",
    "        \n",
    "        # Register hooks on key modules\n",
    "        capture.register_hook(model.embedding, \"after_embedding\")\n",
    "        \n",
    "        # Only register pos_enc hook if it exists and is not rotary\n",
    "        if model.pos_enc is not None and pe_type not in [\"none\", \"rotary\"]:\n",
    "            capture.register_hook(model.pos_enc, \"after_pos_enc\")\n",
    "        \n",
    "        # Register hooks on each encoder block\n",
    "        for block_idx, block in enumerate(model.encoder.blocks):\n",
    "            capture.register_hook(block, f\"after_block_{block_idx}\")\n",
    "        \n",
    "        # Initialize batch_activations with all possible keys\n",
    "        possible_keys = [\"after_embedding\"]\n",
    "        if model.pos_enc is not None and pe_type not in [\"none\", \"rotary\"]:\n",
    "            possible_keys.append(\"after_pos_enc\")\n",
    "        possible_keys.extend([f\"after_block_{i}\" for i in range(len(model.encoder.blocks))])\n",
    "        \n",
    "        batch_activations = {key: [] for key in possible_keys}\n",
    "        batch_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(data_loader):\n",
    "                if batch_idx >= num_batches:\n",
    "                    break\n",
    "                \n",
    "                # Unpack batch - handle different formats\n",
    "                if len(batch) == 4:  # Raw format: (tokens_cont, tokens_id, labels, mask)\n",
    "                    tokens_cont, tokens_id, labels, mask = batch\n",
    "                    tokens_cont = tokens_cont.to(device)\n",
    "                    tokens_id = tokens_id.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    _ = model(tokens_cont, tokens_id, mask=mask)\n",
    "                elif len(batch) == 5:  # Raw format with globals: (tokens_cont, tokens_id, globals, mask, labels)\n",
    "                    tokens_cont, tokens_id, globals, mask, labels = batch\n",
    "                    tokens_cont = tokens_cont.to(device)\n",
    "                    tokens_id = tokens_id.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    _ = model(tokens_cont, tokens_id, mask=mask)\n",
    "                else:  # Binned format: (tokens, labels, mask) or (tokens, globals, mask, labels)\n",
    "                    if len(batch) == 3:\n",
    "                        tokens, labels, mask = batch\n",
    "                        tokens = tokens.to(device)\n",
    "                        mask = mask.to(device)\n",
    "                        _ = model(tokens, mask=mask)\n",
    "                    else:\n",
    "                        tokens, globals, mask, labels = batch\n",
    "                        tokens = tokens.to(device)\n",
    "                        mask = mask.to(device)\n",
    "                        _ = model(tokens, mask=mask)\n",
    "                \n",
    "                # Store activations\n",
    "                for key in capture.activations:\n",
    "                    if key in batch_activations:\n",
    "                        batch_activations[key].append(capture.activations[key])\n",
    "                \n",
    "                batch_labels.append(labels.cpu())\n",
    "                capture.clear()\n",
    "        \n",
    "        # Concatenate batches\n",
    "        representations = {}\n",
    "        for key, act_list in batch_activations.items():\n",
    "            if len(act_list) > 0:\n",
    "                representations[key] = torch.cat(act_list, dim=0)\n",
    "        \n",
    "        all_representations[pe_type] = representations\n",
    "        all_labels[pe_type] = torch.cat(batch_labels, dim=0)\n",
    "        \n",
    "        # Clean up hooks\n",
    "        capture.remove_hooks()\n",
    "        \n",
    "        print(f\"‚úì Extracted representations for {pe_type}\")\n",
    "        print(f\"  Layers captured: {list(representations.keys())}\")\n",
    "    \n",
    "    return all_representations, all_labels\n",
    "\n",
    "print(\"‚úì Extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract representations from validation set\n",
    "print(\"Extracting layer-by-layer representations...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "representations, labels = extract_layer_representations(models_info, val_dl, num_batches=5)\n",
    "\n",
    "print(\"\\n‚úì Extraction complete!\")\n",
    "print(f\"\\nModels analyzed: {list(representations.keys())}\")\n",
    "print(f\"Sample counts per model: {labels[list(labels.keys())[0]].shape[0]}\")\n",
    "\n",
    "# Display shape information\n",
    "sample_pe_type = list(representations.keys())[0]\n",
    "print(f\"\\nExample shapes for '{sample_pe_type}':\")\n",
    "for layer_name, tensor in representations[sample_pe_type].items():\n",
    "    print(f\"  {layer_name}: {tensor.shape} [batch, tokens, dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893a816",
   "metadata": {},
   "source": [
    "## 4. Visualize Representations in Model Dimensions\n",
    "\n",
    "Analyze how representations evolve through the network by looking at:\n",
    "- Token-wise L2 norms (representation magnitude)\n",
    "- Pairwise token similarities (cosine similarity)\n",
    "- How these differ across positional encoding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token-wise L2 norms across layers\n",
    "def compute_l2_norms(representations):\n",
    "    \"\"\"Compute L2 norm for each token across all layers.\"\"\"\n",
    "    norms = {}\n",
    "    \n",
    "    for pe_type, layers in representations.items():\n",
    "        norms[pe_type] = {}\n",
    "        for layer_name, tensor in layers.items():\n",
    "            # tensor shape: [B, T, D]\n",
    "            # Compute L2 norm along dimension axis\n",
    "            l2 = torch.norm(tensor, p=2, dim=-1)  # [B, T]\n",
    "            norms[pe_type][layer_name] = l2.mean(dim=0).numpy()  # Average over batch -> [T]\n",
    "    \n",
    "    return norms\n",
    "\n",
    "# Compute norms\n",
    "print(\"Computing L2 norms...\")\n",
    "l2_norms = compute_l2_norms(representations)\n",
    "print(\"‚úì L2 norms computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot L2 norms evolution through layers\n",
    "# Select key layers to visualize\n",
    "sample_pe = list(l2_norms.keys())[0]\n",
    "num_blocks = len([k for k in l2_norms[sample_pe] if 'block' in k])\n",
    "\n",
    "layers_to_plot = ['after_embedding']\n",
    "if 'after_pos_enc' in l2_norms[sample_pe]:\n",
    "    layers_to_plot.append('after_pos_enc')\n",
    "# Add first, middle, and last block\n",
    "layers_to_plot.extend(['after_block_0', f'after_block_{num_blocks//2}', f'after_block_{num_blocks-1}'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "pe_types_ordered = ['none', 'sinusoidal', 'learned', 'rotary']\n",
    "pe_types_available = [pt for pt in pe_types_ordered if pt in l2_norms]\n",
    "\n",
    "for idx, pe_type in enumerate(pe_types_available):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for layer_name in layers_to_plot:\n",
    "        if layer_name in l2_norms[pe_type]:\n",
    "            norms = l2_norms[pe_type][layer_name]\n",
    "            ax.plot(norms, label=layer_name, marker='o', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Token Position', fontsize=12)\n",
    "    ax.set_ylabel('L2 Norm', fontsize=12)\n",
    "    ax.set_title(f'{pe_type.capitalize()} - Token Representation Magnitude', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(pe_types_available), 4):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- L2 norms show how representation magnitude changes through layers\")\n",
    "print(\"- Different PE types may show different patterns of token differentiation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise token similarity matrices\n",
    "def compute_similarity_matrix(tensor):\n",
    "    \"\"\"Compute cosine similarity between all token pairs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.Tensor\n",
    "        Shape [B, T, D]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Similarity matrix [T, T] averaged over batch\n",
    "    \"\"\"\n",
    "    # Average over batch\n",
    "    token_vecs = tensor.mean(dim=0)  # [T, D]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    from torch.nn.functional import cosine_similarity\n",
    "    \n",
    "    T = token_vecs.shape[0]\n",
    "    sim_matrix = np.zeros((T, T))\n",
    "    \n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == j:\n",
    "                sim_matrix[i, j] = 1.0\n",
    "            else:\n",
    "                sim = cosine_similarity(\n",
    "                    token_vecs[i:i+1], \n",
    "                    token_vecs[j:j+1], \n",
    "                    dim=1\n",
    "                ).item()\n",
    "                sim_matrix[i, j] = sim\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "print(\"‚úì Similarity computation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices at key layers\n",
    "# Compare after positional encoding is added\n",
    "\n",
    "# Ensure pe_types_available is defined\n",
    "pe_types_ordered = ['none', 'sinusoidal', 'learned', 'rotary']\n",
    "pe_types_available = [pt for pt in pe_types_ordered if pt in representations]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "layer_to_compare = 'after_pos_enc'  # or 'after_embedding' if no pos_enc\n",
    "\n",
    "for idx, pe_type in enumerate(pe_types_available):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Find appropriate layer\n",
    "    if layer_to_compare in representations[pe_type]:\n",
    "        layer_tensor = representations[pe_type][layer_to_compare]\n",
    "        actual_layer = layer_to_compare\n",
    "    elif 'after_embedding' in representations[pe_type]:\n",
    "        layer_tensor = representations[pe_type]['after_embedding']\n",
    "        actual_layer = 'after_embedding'\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{pe_type}\\nNo data', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = compute_similarity_matrix(layer_tensor)\n",
    "    \n",
    "    # Plot\n",
    "    im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=-1, vmax=1, aspect='auto')\n",
    "    ax.set_xlabel('Token Index', fontsize=12)\n",
    "    ax.set_ylabel('Token Index', fontsize=12)\n",
    "    ax.set_title(f'{pe_type.capitalize()} - Token Similarity\\n{actual_layer}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Cosine Similarity')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(pe_types_available), 4):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Diagonal is always 1.0 (token similarity with itself)\")\n",
    "print(\"- Off-diagonal patterns show how PE creates token differentiation\")\n",
    "print(\"- 'None' should show more uniform similarity (less position info)\")\n",
    "print(\"- Sinusoidal/Learned/Rotary should show position-dependent patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bd93b",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction: PCA and t-SNE\n",
    "\n",
    "Project high-dimensional representations to 2D/3D to visualize:\n",
    "- Class separability (4t vs background)\n",
    "- How separation emerges through layers\n",
    "- Differences between positional encoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"‚úì Dimensionality reduction imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a92ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for dimensionality reduction\n",
    "def prepare_for_reduction(tensor, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Prepare tensor for dimensionality reduction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : torch.Tensor\n",
    "        Shape [B, T, D]\n",
    "    pooling : str\n",
    "        'mean' or 'cls' (use first token)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Shape [B, D]\n",
    "    \"\"\"\n",
    "    if pooling == 'mean':\n",
    "        # Average over tokens\n",
    "        pooled = tensor.mean(dim=1)  # [B, D]\n",
    "    elif pooling == 'cls':\n",
    "        # Use first token (CLS)\n",
    "        pooled = tensor[:, 0, :]  # [B, D]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling: {pooling}\")\n",
    "    \n",
    "    return pooled.numpy()\n",
    "\n",
    "print(\"‚úì Data preparation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24529c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization - compare final layer across all PE types\n",
    "\n",
    "# Ensure pe_types_available and num_blocks are defined\n",
    "pe_types_ordered = ['none', 'sinusoidal', 'learned', 'rotary']\n",
    "pe_types_available = [pt for pt in pe_types_ordered if pt in representations]\n",
    "\n",
    "# Determine num_blocks from first available model\n",
    "if pe_types_available:\n",
    "    sample_pe = pe_types_available[0]\n",
    "    num_blocks = len([k for k in representations[sample_pe] if 'block' in k])\n",
    "    final_layer = f'after_block_{num_blocks-1}'\n",
    "else:\n",
    "    final_layer = 'after_block_0'  # fallback\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, pe_type in enumerate(pe_types_available):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if final_layer in representations[pe_type]:\n",
    "        # Get final layer representations\n",
    "        tensor = representations[pe_type][final_layer]\n",
    "        X = prepare_for_reduction(tensor, pooling='mean')\n",
    "        y = labels[pe_type].numpy()\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        # Plot by class\n",
    "        for class_idx in [0, 1]:\n",
    "            mask = y == class_idx\n",
    "            class_name = '4t (signal)' if class_idx == 1 else 'Background'\n",
    "            ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                      label=class_name, alpha=0.6, s=20)\n",
    "        \n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)', fontsize=12)\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)', fontsize=12)\n",
    "        ax.set_title(f'{pe_type.capitalize()} - PCA of Final Layer\\nTotal variance: {pca.explained_variance_ratio_[:2].sum():.1%}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{pe_type}\\nNo data', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(pe_types_available), 4):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPCA shows linear separability of classes\")\n",
    "print(\"Better separation = easier classification task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization - focuses on local structure\n",
    "print(\"Computing t-SNE (this may take a minute)...\")\n",
    "\n",
    "# Ensure pe_types_available and final_layer are defined\n",
    "pe_types_ordered = ['none', 'sinusoidal', 'learned', 'rotary']\n",
    "pe_types_available = [pt for pt in pe_types_ordered if pt in representations]\n",
    "\n",
    "# Determine num_blocks and final_layer\n",
    "if pe_types_available:\n",
    "    sample_pe = pe_types_available[0]\n",
    "    num_blocks = len([k for k in representations[sample_pe] if 'block' in k])\n",
    "    final_layer = f'after_block_{num_blocks-1}'\n",
    "else:\n",
    "    final_layer = 'after_block_0'  # fallback\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, pe_type in enumerate(pe_types_available):\n",
    "    print(f\"  Processing {pe_type}...\")\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    if final_layer in representations[pe_type]:\n",
    "        # Get final layer representations\n",
    "        tensor = representations[pe_type][final_layer]\n",
    "        X = prepare_for_reduction(tensor, pooling='mean')\n",
    "        y = labels[pe_type].numpy()\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        \n",
    "        # Plot by class\n",
    "        for class_idx in [0, 1]:\n",
    "            mask = y == class_idx\n",
    "            class_name = '4t (signal)' if class_idx == 1 else 'Background'\n",
    "            ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], \n",
    "                      label=class_name, alpha=0.6, s=20)\n",
    "        \n",
    "        ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "        ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "        ax.set_title(f'{pe_type.capitalize()} - t-SNE of Final Layer', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'{pe_type}\\nNo data', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(pe_types_available), 4):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì t-SNE complete\")\n",
    "print(\"t-SNE reveals local clustering structure\")\n",
    "print(\"Well-separated clusters indicate better learned representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution through layers - show how PCA separation improves\n",
    "# Pick one PE type to show layer evolution\n",
    "\n",
    "# Ensure pe_types_available and num_blocks are defined\n",
    "pe_types_ordered = ['none', 'sinusoidal', 'learned', 'rotary']\n",
    "pe_types_available = [pt for pt in pe_types_ordered if pt in representations]\n",
    "\n",
    "if not pe_types_available:\n",
    "    print(\"No models available for layer evolution visualization\")\n",
    "else:\n",
    "    pe_for_evolution = 'sinusoidal' if 'sinusoidal' in representations else pe_types_available[0]\n",
    "    \n",
    "    # Determine num_blocks\n",
    "    num_blocks = len([k for k in representations[pe_for_evolution] if 'block' in k])\n",
    "    \n",
    "    # Select layers to show evolution\n",
    "    evolution_layers = ['after_embedding']\n",
    "    if 'after_pos_enc' in representations[pe_for_evolution]:\n",
    "        evolution_layers.append('after_pos_enc')\n",
    "    evolution_layers.extend([f'after_block_{i}' for i in [0, num_blocks//2, num_blocks-1]])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, layer_name in enumerate(evolution_layers[:6]):  # Max 6 subplots\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if layer_name in representations[pe_for_evolution]:\n",
    "            tensor = representations[pe_for_evolution][layer_name]\n",
    "            X = prepare_for_reduction(tensor, pooling='mean')\n",
    "            y = labels[pe_for_evolution].numpy()\n",
    "            \n",
    "            # Apply PCA\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X)\n",
    "            \n",
    "            # Plot by class\n",
    "            for class_idx in [0, 1]:\n",
    "                mask = y == class_idx\n",
    "                class_name = '4t' if class_idx == 1 else 'Bkg'\n",
    "                ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                          label=class_name, alpha=0.6, s=15)\n",
    "            \n",
    "            var_explained = pca.explained_variance_ratio_[:2].sum()\n",
    "            ax.set_xlabel('PC1', fontsize=10)\n",
    "            ax.set_ylabel('PC2', fontsize=10)\n",
    "            ax.set_title(f'{layer_name}\\nVar: {var_explained:.1%}', fontsize=12, fontweight='bold')\n",
    "            ax.legend(fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Layer Evolution - {pe_for_evolution.capitalize()} PE', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nShowing how class separation evolves through layers for {pe_for_evolution}\")\n",
    "    print(\"Watch how the two classes become more separable in deeper layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba99fe",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison\n",
    "\n",
    "Run inference on all models and compare their classification performance to quantify why positional encoding matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b81966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all models\n",
    "from thesis_ml.reports.inference.classification import run_classification_inference\n",
    "\n",
    "print(\"Running inference on all models...\")\n",
    "print(\"This will compute predictions and metrics on the test set.\\n\")\n",
    "\n",
    "# Prepare models list\n",
    "models_list = []\n",
    "for _pe_type, info in models_info.items():\n",
    "    models_list.append((info['run_id'], info['cfg'], info['model']))\n",
    "\n",
    "# Run inference\n",
    "try:\n",
    "    inference_results = run_classification_inference(\n",
    "        models=models_list,\n",
    "        dataset_cfg=sample_cfg,\n",
    "        split=\"test\",\n",
    "        inference_cfg={\n",
    "            \"batch_size\": 512,\n",
    "            \"autocast\": False,\n",
    "            \"seed\": 42,\n",
    "            \"max_samples\": None,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Inference complete!\")\n",
    "    print(f\"Models evaluated: {list(inference_results.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Inference failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    inference_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d676101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance metrics table\n",
    "import pandas as pd\n",
    "\n",
    "if inference_results:\n",
    "    metrics_data = []\n",
    "    \n",
    "    for run_id, metrics in inference_results.items():\n",
    "        # Find corresponding PE type\n",
    "        pe_type = None\n",
    "        for pt, info in models_info.items():\n",
    "            if info['run_id'] == run_id:\n",
    "                pe_type = pt\n",
    "                break\n",
    "        \n",
    "        if pe_type and isinstance(metrics, dict):\n",
    "            metrics_data.append({\n",
    "                'Positional Encoding': pe_type.capitalize(),\n",
    "                'Accuracy': f\"{metrics.get('accuracy', 0.0):.4f}\",\n",
    "                'Precision': f\"{metrics.get('precision', 0.0):.4f}\",\n",
    "                'Recall': f\"{metrics.get('recall', 0.0):.4f}\",\n",
    "                'F1': f\"{metrics.get('f1', 0.0):.4f}\",\n",
    "                'AUROC': f\"{metrics.get('auroc', 0.0):.4f}\",\n",
    "            })\n",
    "    \n",
    "    if metrics_data:\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        metrics_df = metrics_df.sort_values('AUROC', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PERFORMANCE COMPARISON - 4t vs Background Classification\")\n",
    "        print(\"=\"*80)\n",
    "        print(metrics_df.to_string(index=False))\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Highlight best performance\n",
    "        best_pe = metrics_df.iloc[0]['Positional Encoding']\n",
    "        best_auroc = metrics_df.iloc[0]['AUROC']\n",
    "        print(f\"\\nüèÜ Best performing: {best_pe} (AUROC: {best_auroc})\")\n",
    "    else:\n",
    "        print(\"No metrics data available\")\n",
    "else:\n",
    "    print(\"No inference results available. Please run inference first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb37817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "if inference_results:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    has_data = False\n",
    "    for run_id, metrics in inference_results.items():\n",
    "        # Find corresponding PE type\n",
    "        pe_type = None\n",
    "        for pt, info in models_info.items():\n",
    "            if info['run_id'] == run_id:\n",
    "                pe_type = pt\n",
    "                break\n",
    "        \n",
    "        if pe_type and isinstance(metrics, dict) and 'roc_curves' in metrics:\n",
    "            roc_curves = metrics['roc_curves']\n",
    "            # Try class 1 (4t signal), fallback to first available class\n",
    "            class_key = 1 if 1 in roc_curves else (list(roc_curves.keys())[0] if roc_curves else None)\n",
    "            \n",
    "            if class_key is not None:\n",
    "                roc_data = roc_curves[class_key]\n",
    "                if isinstance(roc_data, dict) and 'fpr' in roc_data and 'tpr' in roc_data:\n",
    "                    fpr = roc_data['fpr']\n",
    "                    tpr = roc_data['tpr']\n",
    "                    auroc = metrics.get('auroc', 0.0)\n",
    "                    \n",
    "                    ax.plot(fpr, tpr, label=f'{pe_type.capitalize()} (AUROC={auroc:.4f})', \n",
    "                            linewidth=2.5, alpha=0.8)\n",
    "                    has_data = True\n",
    "    \n",
    "    if has_data:\n",
    "        # Plot diagonal\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5, label='Random')\n",
    "        \n",
    "        ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "        ax.set_title('ROC Curves - Positional Encoding Comparison\\n4t vs Background Classification', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nROC curves show the trade-off between true positive rate and false positive rate\")\n",
    "        print(\"Higher curve = better classification performance\")\n",
    "    else:\n",
    "        print(\"No ROC curve data available\")\n",
    "else:\n",
    "    print(\"No inference results available. Please run inference first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction score distributions\n",
    "if inference_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    pe_types_with_scores = []\n",
    "    for run_id, metrics in inference_results.items():\n",
    "        for pt, info in models_info.items():\n",
    "            if info['run_id'] == run_id:\n",
    "                pe_types_with_scores.append((pt, metrics))\n",
    "                break\n",
    "    \n",
    "    for idx, (pe_type, metrics) in enumerate(pe_types_with_scores[:4]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        if isinstance(metrics, dict) and 'per_event_scores' in metrics and 'per_event_labels' in metrics:\n",
    "            scores = metrics['per_event_scores']\n",
    "            true_labels = metrics['per_event_labels']\n",
    "            \n",
    "            # Ensure scores and labels are numpy arrays\n",
    "            if isinstance(scores, torch.Tensor):\n",
    "                scores = scores.numpy()\n",
    "            if isinstance(true_labels, torch.Tensor):\n",
    "                true_labels = true_labels.numpy()\n",
    "            \n",
    "            # Ensure they are 1D arrays (flatten if needed)\n",
    "            scores = np.asarray(scores).flatten()\n",
    "            true_labels = np.asarray(true_labels).flatten()\n",
    "            \n",
    "            # Plot distributions for each class\n",
    "            for class_idx in [0, 1]:\n",
    "                class_mask = true_labels == class_idx\n",
    "                # Check if mask is array-like and has any True values\n",
    "                if hasattr(class_mask, 'sum') and class_mask.sum() > 0:\n",
    "                    class_scores = scores[class_mask]\n",
    "                    class_name = '4t (signal)' if class_idx == 1 else 'Background'\n",
    "                    ax.hist(class_scores, bins=50, alpha=0.6, label=class_name, density=True)\n",
    "            \n",
    "            ax.set_xlabel('Predicted Signal Probability', fontsize=12)\n",
    "            ax.set_ylabel('Density', fontsize=12)\n",
    "            ax.set_title(f'{pe_type.capitalize()} - Score Distribution', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.axvline(0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Threshold=0.5')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'{pe_type}\\nNo score data', \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(pe_types_with_scores), 4):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nScore distributions show how well the model separates the two classes\")\n",
    "    print(\"Good separation = distinct peaks for signal and background\")\n",
    "else:\n",
    "    print(\"No inference results available. Please run inference first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905ab26",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "This notebook demonstrates how different positional encoding strategies affect transformer-based classification:\n",
    "\n",
    "#### **Positional Encoding Patterns**\n",
    "- **Sinusoidal**: Fixed wave patterns with varying frequencies across dimensions\n",
    "- **Learned**: Trainable embeddings optimized during training\n",
    "- **Rotary (RoPE)**: Applied in attention mechanism (not additive to embeddings)\n",
    "- **None**: No positional information (permutation invariant)\n",
    "\n",
    "#### **Representation Evolution**\n",
    "- Positional encodings create distinct token relationships early in the network\n",
    "- Token similarity patterns differ significantly between PE types\n",
    "- Class separation emerges and strengthens through transformer layers\n",
    "\n",
    "#### **Performance Impact**\n",
    "The metrics table above shows quantitative differences in classification performance. Key observations:\n",
    "- Models with positional encoding generally outperform \"none\"\n",
    "- Different PE strategies (sinusoidal vs learned vs rotary) show varying effectiveness\n",
    "- PCA/t-SNE visualizations correlate with classification performance\n",
    "\n",
    "#### **Why Positional Encoding Helps**\n",
    "1. **Token Differentiation**: Adds unique information to each token position\n",
    "2. **Sequence Structure**: Allows the model to learn position-dependent patterns\n",
    "3. **Better Representations**: Creates more separable feature spaces (shown in PCA/t-SNE)\n",
    "4. **Improved Classification**: Translates to higher AUROC and accuracy\n",
    "\n",
    "### Particle Physics Context\n",
    "For particle physics events, positional encoding can capture:\n",
    "- Particle ordering (e.g., by pT, Œ∑, œÜ)\n",
    "- Detector geometry information\n",
    "- Spatial relationships between particles\n",
    "\n",
    "However, true permutation invariance (PE=none) may be preferred when particle order is arbitrary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
