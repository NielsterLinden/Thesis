{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aafa91",
   "metadata": {},
   "source": [
    "# üéØ Understanding the Inference Pipeline\n",
    "\n",
    "This notebook walks through the inference phase step-by-step:\n",
    "1. **Loading a trained model** from a run\n",
    "2. **Getting data** (batches from dataloader)\n",
    "3. **Running inference** (forward pass through the model)\n",
    "4. **Computing metrics** (accuracy, AUROC, confusion matrix, etc.)\n",
    "5. **Creating plots** (ROC curves, confusion matrices, score distributions)\n",
    "\n",
    "Let's explore how classification inference works! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from thesis_ml.reports.inference.classification import run_classification_inference\n",
    "from thesis_ml.reports.inference.classification_metrics import compute_classification_metrics\n",
    "from thesis_ml.reports.plots.classification import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_metrics_comparison,\n",
    "    plot_roc_curves,\n",
    "    plot_score_distributions,\n",
    ")\n",
    "\n",
    "# Inference utilities\n",
    "\n",
    "print(\"‚úÖ All imports loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f59ce",
   "metadata": {},
   "source": [
    "## Step 1: Load a Trained Model üé®\n",
    "\n",
    "First, we need to load a trained classifier model from a run. The `load_model_from_run` function:\n",
    "- Finds the run directory\n",
    "- Loads the config (from `.hydra/config.yaml` or `cfg.yaml`)\n",
    "- Builds the model architecture\n",
    "- Loads the trained weights (from `best_val.pt` or `model.pt`)\n",
    "- Puts the model in eval mode\n",
    "\n",
    "Let's pick a run and load it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Get repo root - handle both cases: running from repo root or from notebooks/\n",
    "cwd = Path.cwd()\n",
    "repo_root = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
    "\n",
    "output_root = repo_root / \"outputs\"\n",
    "print(f\"üìÅ Current directory: {cwd.absolute()}\")\n",
    "print(f\"üìÅ Using output_root: {output_root.absolute()}\")\n",
    "\n",
    "# Verify the path exists\n",
    "if not output_root.exists():\n",
    "    raise FileNotFoundError(f\"Output directory not found: {output_root.absolute()}\\n\"\n",
    "                          f\"Make sure you're running from the repo root or notebooks/ directory\")\n",
    "\n",
    "# Pick a classifier run (one with best_val.pt)\n",
    "# You can change this to any classifier run ID you have\n",
    "run_id = \"run_20251113-113008_run\"  # Change this to your run!\n",
    "\n",
    "# Debug: Check if the run directory and config file exist\n",
    "run_dir = output_root / \"runs\" / run_id\n",
    "hydra_config = run_dir / \".hydra\" / \"config.yaml\"\n",
    "cfg_yaml = run_dir / \"cfg.yaml\"\n",
    "\n",
    "print(\"\\nüîç Debugging paths:\")\n",
    "print(f\"   Run directory: {run_dir.absolute()}\")\n",
    "print(f\"   Run directory exists: {run_dir.exists()}\")\n",
    "print(f\"   .hydra/config.yaml: {hydra_config.absolute()}\")\n",
    "print(f\"   .hydra/config.yaml exists: {hydra_config.exists()}\")\n",
    "print(f\"   cfg.yaml: {cfg_yaml.absolute()}\")\n",
    "print(f\"   cfg.yaml exists: {cfg_yaml.exists()}\")\n",
    "\n",
    "# Load the model\n",
    "print(f\"\\nüì¶ Loading model from run: {run_id}\")\n",
    "\n",
    "# First, load the config manually so we can override the data path\n",
    "from thesis_ml.utils.paths import resolve_run_dir  # noqa: E402\n",
    "\n",
    "run_dir = resolve_run_dir(run_id, output_root)\n",
    "hydra_cfg_path = run_dir / \".hydra\" / \"config.yaml\"\n",
    "if hydra_cfg_path.exists():\n",
    "    cfg = OmegaConf.load(str(hydra_cfg_path))\n",
    "else:\n",
    "    cfg_path = run_dir / \"cfg.yaml\"\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing .hydra/config.yaml or cfg.yaml in {run_dir}\")\n",
    "    cfg = OmegaConf.load(str(cfg_path))\n",
    "\n",
    "# Override data path to use local file\n",
    "local_data_path = Path(\"C:/Users/niels/Projects/Thesis-Code/Data/4tops_splitted.h5\")\n",
    "if local_data_path.exists():\n",
    "    print(f\"üìÅ Overriding data path to use local file: {local_data_path}\")\n",
    "    # Override the data path - it's stored as ${env.data_root}/4tops_splitted.h5\n",
    "    # We can either override env.data_root or directly set data.path\n",
    "    cfg.data.path = str(local_data_path)\n",
    "    # Also update env.data_root for consistency\n",
    "    if hasattr(cfg, \"env\"):\n",
    "        cfg.env.data_root = str(local_data_path.parent)\n",
    "    print(f\"   Updated data.path: {cfg.data.path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Local data file not found at {local_data_path}\")\n",
    "    print(f\"   Using original path: {cfg.data.get('path', 'N/A')}\")\n",
    "\n",
    "# Now load the model with the updated config\n",
    "from thesis_ml.architectures.transformer_classifier.base import build_from_config  # noqa: E402\n",
    "from thesis_ml.data.h5_loader import make_classification_dataloaders  # noqa: E402\n",
    "from thesis_ml.training_loops.transformer_classifier import _gather_meta  # noqa: E402\n",
    "\n",
    "# Resolve device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Populate meta if missing (needed for model building)\n",
    "if not hasattr(cfg, \"meta\") or cfg.meta is None:\n",
    "    # Create dataloaders temporarily to get meta\n",
    "    train_dl, val_dl, test_dl, meta = make_classification_dataloaders(cfg)\n",
    "    _gather_meta(cfg, meta)\n",
    "\n",
    "model = build_from_config(cfg, cfg.meta).to(device)\n",
    "\n",
    "# Load weights\n",
    "best_val_path = run_dir / \"best_val.pt\"\n",
    "model_pt_path = run_dir / \"model.pt\"\n",
    "if best_val_path.exists():\n",
    "    weights_path = best_val_path\n",
    "elif model_pt_path.exists():\n",
    "    weights_path = model_pt_path\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing best_val.pt or model.pt in {run_dir}\")\n",
    "\n",
    "checkpoint = torch.load(str(weights_path), map_location=device, weights_only=False)\n",
    "state_dict = checkpoint[\"model_state_dict\"] if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint else checkpoint\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(f\"   Model in eval mode: {not model.training}\")\n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(\"\\nüìä Model Parameters:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Non-trainable parameters: {non_trainable_params:,}\")\n",
    "\n",
    "# Check if it's a classifier\n",
    "if hasattr(cfg, \"classifier\"):\n",
    "    print(\"   ‚úÖ This is a classifier model!\")\n",
    "    if hasattr(cfg, \"meta\") and hasattr(cfg.meta, \"n_classes\"):\n",
    "        print(f\"   Number of classes: {cfg.meta.n_classes}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  This might be an autoencoder, not a classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed5e9a",
   "metadata": {},
   "source": [
    "## Step 2: Get a Batch of Data üìä\n",
    "\n",
    "Now let's get a single batch from the validation set to see what the data looks like. The batch format depends on whether we're using:\n",
    "- **Raw format**: `(tokens_cont, tokens_id, globals, mask, label)` - 5 items\n",
    "- **Binned format**: `(integer_tokens, globals_ints, mask, label)` - 4 items\n",
    "\n",
    "Let's peek at one batch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ba6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from validation set\n",
    "# Note: For classification, we need to use make_classification_dataloaders, not get_example_batch\n",
    "print(\"üìä Getting a batch from validation set...\")\n",
    "from thesis_ml.data.h5_loader import make_classification_dataloaders  # noqa: E402\n",
    "\n",
    "train_dl, val_dl, test_dl, _meta = make_classification_dataloaders(cfg)\n",
    "batch = next(iter(val_dl))\n",
    "\n",
    "print(\"‚úÖ Batch retrieved!\")\n",
    "print(f\"   Batch type: {type(batch)}\")\n",
    "print(f\"   Number of items in batch: {len(batch)}\")\n",
    "\n",
    "# Unpack based on format\n",
    "if len(batch) == 5:\n",
    "    tokens_cont, tokens_id, globals, mask, label = batch\n",
    "    print(\"   Format: Raw (5 items)\")\n",
    "    print(f\"   tokens_cont shape: {tokens_cont.shape}\")  # [B, T, 4]\n",
    "    print(f\"   tokens_id shape: {tokens_id.shape}\")     # [B, T]\n",
    "    print(f\"   globals shape: {globals.shape}\")          # [B, G]\n",
    "    print(f\"   mask shape: {mask.shape}\")                # [B, T]\n",
    "    print(f\"   label shape: {label.shape}\")             # [B]\n",
    "elif len(batch) == 4:\n",
    "    integer_tokens, globals_ints, mask, label = batch\n",
    "    print(\"   Format: Binned (4 items)\")\n",
    "    print(f\"   integer_tokens shape: {integer_tokens.shape}\")  # [B, T]\n",
    "    print(f\"   globals_ints shape: {globals_ints.shape}\")     # [B, G]\n",
    "    print(f\"   mask shape: {mask.shape}\")                     # [B, T]\n",
    "    print(f\"   label shape: {label.shape}\")                   # [B]\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected batch format with {len(batch)} items\")\n",
    "\n",
    "# Print label info\n",
    "print(f\"\\n   Label values (first 10): {label[:10].tolist()}\")\n",
    "print(f\"   Unique labels: {torch.unique(label).tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15df91",
   "metadata": {},
   "source": [
    "## Step 3: Run Inference on a Single Batch üîÆ\n",
    "\n",
    "Now let's do a forward pass through the model! This is the core of inference:\n",
    "1. Move batch to device (GPU/CPU)\n",
    "2. Run model forward pass (with `torch.no_grad()` for efficiency)\n",
    "3. Get logits (raw model outputs)\n",
    "4. Convert to probabilities using softmax\n",
    "5. Get predictions (argmax of probabilities)\n",
    "\n",
    "Let's see it in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device and set to eval mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Run inference on the batch\n",
    "print(\"üîÆ Running inference on batch...\")\n",
    "with torch.no_grad():\n",
    "    # Move batch to device\n",
    "    if len(batch) == 5:\n",
    "        tokens_cont, tokens_id, globals, mask, label = batch\n",
    "        tokens_cont = tokens_cont.to(device)\n",
    "        tokens_id = tokens_id.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(tokens_cont, tokens_id, mask=mask)\n",
    "    else:  # binned format\n",
    "        integer_tokens, globals_ints, mask, label = batch\n",
    "        integer_tokens = integer_tokens.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(integer_tokens, mask=mask)\n",
    "\n",
    "print(\"‚úÖ Forward pass complete!\")\n",
    "print(f\"   Logits shape: {logits.shape}\")  # [B, n_classes]\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(f\"   Probabilities shape: {probs.shape}\")  # [B, n_classes]\n",
    "print(f\"   Probabilities sum to 1: {torch.allclose(probs.sum(dim=-1), torch.ones(probs.shape[0], device=device))}\")\n",
    "\n",
    "# Get predictions\n",
    "preds = probs.argmax(dim=-1)\n",
    "print(f\"   Predictions shape: {preds.shape}\")  # [B]\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n   First 5 examples:\")\n",
    "for i in range(min(5, len(label))):\n",
    "    true_label = label[i].item()\n",
    "    pred_label = preds[i].item()\n",
    "    confidence = probs[i, pred_label].item()\n",
    "    print(f\"     Example {i}: True={true_label}, Pred={pred_label}, Confidence={confidence:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50c5da",
   "metadata": {},
   "source": [
    "## Step 4: Run Full Inference on Dataset üéØ\n",
    "\n",
    "Now let's run inference on the entire validation set! This is what `run_classification_inference` does:\n",
    "1. Creates a dataloader for the specified split\n",
    "2. Iterates through all batches\n",
    "3. Accumulates logits, labels, and probabilities\n",
    "4. Computes comprehensive metrics\n",
    "\n",
    "Let's do it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98146480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model in the format expected by run_classification_inference\n",
    "# It expects: list of (run_id, cfg, model) tuples\n",
    "models = [(run_id, cfg, model)]\n",
    "\n",
    "# Run inference on validation set (with limited samples for speed)\n",
    "print(\"üéØ Running full inference on validation set...\")\n",
    "print(\"   (Using max_samples=1000 for speed - remove this in production!)\")\n",
    "\n",
    "inference_results = run_classification_inference(\n",
    "    models=models,\n",
    "    dataset_cfg=cfg,\n",
    "    split=\"val\",\n",
    "    inference_cfg={\n",
    "        \"batch_size\": 512,\n",
    "        \"autocast\": False,\n",
    "        \"seed\": 42,\n",
    "        \"max_samples\": 1000,  # Limit for demo - remove in production!\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Inference complete!\")\n",
    "print(f\"   Results keys: {list(inference_results.keys())}\")\n",
    "\n",
    "# Show metrics for our run\n",
    "if run_id in inference_results:\n",
    "    metrics = inference_results[run_id]\n",
    "    print(f\"\\nüìä Metrics for {run_id}:\")\n",
    "    print(f\"   Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "    print(f\"   AUROC: {metrics.get('auroc', 'N/A')}\")\n",
    "    print(f\"   Precision (macro): {metrics.get('precision_macro', 0):.4f}\")\n",
    "    print(f\"   Recall (macro): {metrics.get('recall_macro', 0):.4f}\")\n",
    "    print(f\"   F1 (macro): {metrics.get('f1_macro', 0):.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = metrics.get('confusion_matrix', [])\n",
    "    if cm:\n",
    "        print(\"\\n   Confusion Matrix:\")\n",
    "        for _i, row in enumerate(cm):\n",
    "            print(f\"     {row}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df6d3d",
   "metadata": {},
   "source": [
    "## Step 5: Understanding the Metrics Computation üìà\n",
    "\n",
    "Let's manually compute metrics on a small batch to see how it works! The `compute_classification_metrics` function:\n",
    "1. Concatenates all batches\n",
    "2. Computes predictions (argmax of probabilities)\n",
    "3. Calculates accuracy, precision, recall, F1\n",
    "4. Computes AUROC (for binary or multi-class)\n",
    "5. Builds confusion matrix\n",
    "6. Computes ROC and PR curves\n",
    "\n",
    "Let's peek under the hood!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually compute metrics on a few batches to understand the process\n",
    "from thesis_ml.data.h5_loader import make_classification_dataloaders\n",
    "\n",
    "# Create a small dataloader\n",
    "temp_cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n",
    "if hasattr(temp_cfg, \"data\"):\n",
    "    temp_cfg.data.limit_samples = 100  # Just 100 samples for demo\n",
    "train_dl, val_dl, test_dl, meta = make_classification_dataloaders(temp_cfg)\n",
    "\n",
    "print(\"üìà Computing metrics manually on a few batches...\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "# Process first 3 batches\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_dl):\n",
    "        if batch_idx >= 3:  # Just 3 batches for demo\n",
    "            break\n",
    "            \n",
    "        # Unpack and move to device\n",
    "        if len(batch) == 5:\n",
    "            tokens_cont, tokens_id, globals, mask, label = batch\n",
    "            tokens_cont = tokens_cont.to(device)\n",
    "            tokens_id = tokens_id.to(device)\n",
    "            mask = mask.to(device)\n",
    "            label = label.to(device)\n",
    "            logits = model(tokens_cont, tokens_id, mask=mask)\n",
    "        else:\n",
    "            integer_tokens, globals_ints, mask, label = batch\n",
    "            integer_tokens = integer_tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            label = label.to(device)\n",
    "            logits = model(integer_tokens, mask=mask)\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Accumulate (move to CPU to save GPU memory)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(label.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "        \n",
    "        print(f\"   Batch {batch_idx + 1}: {logits.shape[0]} samples\")\n",
    "\n",
    "# Get n_classes\n",
    "n_classes = cfg.meta.n_classes if hasattr(cfg, \"meta\") and hasattr(cfg.meta, \"n_classes\") else meta.get(\"n_classes\", 2)\n",
    "\n",
    "# Now compute metrics\n",
    "print(f\"\\nüî¢ Computing metrics (n_classes={n_classes})...\")\n",
    "metrics = compute_classification_metrics(all_logits, all_labels, all_probs, n_classes)\n",
    "\n",
    "print(\"\\n‚úÖ Metrics computed!\")\n",
    "print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"   AUROC: {metrics.get('auroc', 'N/A')}\")\n",
    "print(f\"   Precision per class: {metrics['precision_per_class']}\")\n",
    "print(f\"   Recall per class: {metrics['recall_per_class']}\")\n",
    "print(f\"   F1 per class: {metrics['f1_per_class']}\")\n",
    "print(f\"   Support per class: {metrics['support_per_class']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b1457",
   "metadata": {},
   "source": [
    "## Step 6: Creating Plots üé®\n",
    "\n",
    "Now let's create the beautiful plots! The plotting functions take the inference results and create:\n",
    "1. **ROC Curves**: Shows true positive rate vs false positive rate\n",
    "2. **Confusion Matrix**: Shows prediction vs true label matrix\n",
    "3. **Metrics Comparison**: Bar chart comparing metrics across models\n",
    "4. **Score Distributions**: Histogram of classifier scores for signal vs background\n",
    "\n",
    "Let's generate them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory for figures\n",
    "figs_dir = Path(\"notebooks/temp_figures\")\n",
    "# figs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Figure config (format and DPI)\n",
    "fig_cfg = {\"fig_format\": \"png\", \"dpi\": 150}\n",
    "\n",
    "print(\"üé® Creating plots...\")\n",
    "\n",
    "# 1. ROC Curves\n",
    "print(\"   1. Plotting ROC curves...\")\n",
    "try:\n",
    "    plot_roc_curves(inference_results, figs_dir, fig_cfg)\n",
    "    print(\"      ‚úÖ ROC curves saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Error: {e}\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "print(\"   2. Plotting confusion matrix...\")\n",
    "try:\n",
    "    plot_confusion_matrix(inference_results, figs_dir, fig_cfg)\n",
    "    print(\"      ‚úÖ Confusion matrix saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Error: {e}\")\n",
    "\n",
    "# 3. Metrics Comparison\n",
    "print(\"   3. Plotting metrics comparison...\")\n",
    "try:\n",
    "    plot_metrics_comparison(inference_results, figs_dir, fig_cfg)\n",
    "    print(\"      ‚úÖ Metrics comparison saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Error: {e}\")\n",
    "\n",
    "# 4. Score Distributions (only for binary classification)\n",
    "print(\"   4. Plotting score distributions...\")\n",
    "try:\n",
    "    plot_score_distributions(inference_results, figs_dir, fig_cfg)\n",
    "    print(\"      ‚úÖ Score distributions saved!\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Error: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All plots saved to: {figs_dir}\")\n",
    "print(\"   Files:\")\n",
    "for f in sorted(figs_dir.glob(\"*.png\")):\n",
    "    print(f\"     - {f.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19d3c9",
   "metadata": {},
   "source": [
    "## Step 7: Visualize the Plots üìä\n",
    "\n",
    "Let's display the plots we just created!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the plots\n",
    "from IPython.display import Image, display\n",
    "\n",
    "figs_dir = Path(\"notebooks/temp_figures\")\n",
    "\n",
    "# Find all PNG files\n",
    "png_files = sorted(figs_dir.glob(\"*.png\"))\n",
    "\n",
    "if png_files:\n",
    "    print(f\"üìä Found {len(png_files)} plot(s):\\n\")\n",
    "    for png_file in png_files:\n",
    "        print(f\"   {png_file.name}\")\n",
    "        display(Image(str(png_file), width=800))\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No plots found. Make sure Step 6 ran successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d26af",
   "metadata": {},
   "source": [
    "## Summary: The Inference Pipeline üéì\n",
    "\n",
    "Here's what we learned about the inference pipeline:\n",
    "\n",
    "### 1. **Model Loading** (`load_model_from_run`)\n",
    "   - Finds run directory and loads config\n",
    "   - Builds model architecture from config\n",
    "   - Loads trained weights\n",
    "   - Sets model to eval mode\n",
    "\n",
    "### 2. **Data Loading** (`get_example_batch` / `make_classification_dataloaders`)\n",
    "   - Creates dataloader for specified split (train/val/test)\n",
    "   - Returns batches in either raw (5 items) or binned (4 items) format\n",
    "   - Each batch contains: tokens, masks, labels\n",
    "\n",
    "### 3. **Forward Pass** (model inference)\n",
    "   - Move batch to device (GPU/CPU)\n",
    "   - Run model forward pass with `torch.no_grad()`\n",
    "   - Get logits (raw outputs)\n",
    "   - Convert to probabilities with softmax\n",
    "   - Get predictions with argmax\n",
    "\n",
    "### 4. **Metrics Computation** (`compute_classification_metrics`)\n",
    "   - Concatenates all batches\n",
    "   - Computes accuracy, precision, recall, F1\n",
    "   - Calculates AUROC (binary or multi-class)\n",
    "   - Builds confusion matrix\n",
    "   - Computes ROC and PR curves\n",
    "\n",
    "### 5. **Plotting** (classification plotting functions)\n",
    "   - **ROC Curves**: TPR vs FPR for different models\n",
    "   - **Confusion Matrix**: Prediction vs true label heatmap\n",
    "   - **Metrics Comparison**: Bar chart of key metrics\n",
    "   - **Score Distributions**: Histogram of classifier scores\n",
    "\n",
    "### The Full Pipeline (`run_classification_inference`)\n",
    "   - Orchestrates all steps above\n",
    "   - Processes multiple models\n",
    "   - Handles batching, device management, autocast\n",
    "   - Returns nested dict: `{run_id: {metrics...}}`\n",
    "\n",
    "### Key Takeaways:\n",
    "- Inference is done in **eval mode** (`model.eval()`)\n",
    "- Use `torch.no_grad()` to save memory and speed\n",
    "- Metrics are computed on **all batches** concatenated\n",
    "- Plots are generated from the metrics dictionary\n",
    "- The pipeline supports both **binary** and **multi-class** classification\n",
    "\n",
    "üéâ You now understand how inference works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60030e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up temporary figures directory\n",
    "# Uncomment to delete the temp figures after viewing\n",
    "# import shutil\n",
    "# if figs_dir.exists():\n",
    "#     shutil.rmtree(figs_dir)\n",
    "#     print(\"üßπ Cleaned up temporary figures directory\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
