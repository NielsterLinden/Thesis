{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c89444",
   "metadata": {},
   "source": [
    "# Positional Encoding and Model Size Analysis\n",
    "\n",
    "This notebook analyzes the results from the `compare_positional_encodings` experiment which compares:\n",
    "- 4 positional encodings: none, sinusoidal, learned, rotary\n",
    "- 3 model sizes: small, medium, base\n",
    "\n",
    "Total runs: 4 Ã— 3 = 12 jobs (note: one run may be incomplete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6921ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Hardcoded paths\n",
    "base_dir = Path(\"C:/Users/niels/Projects/Thesis-Code/Saved models/20251126-150519_compare_positional_encodings\")\n",
    "run_prefix = \"run_20251126-150519_compare_positional_encodings\"\n",
    "data_path = Path(\"C:/Users/niels/Projects/Thesis-Code/Data/4tops_splitted.h5\")\n",
    "num_jobs = 12\n",
    "\n",
    "# Load all runs\n",
    "runs_data = []\n",
    "\n",
    "for job_id in range(num_jobs):\n",
    "    run_dir = base_dir / f\"{run_prefix}_job{job_id}\"\n",
    "    \n",
    "    # Skip if directory doesn't exist\n",
    "    if not run_dir.exists():\n",
    "        print(f\"Warning: {run_dir} not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Read events.jsonl - get last line\n",
    "    events_file = run_dir / \"facts\" / \"events.jsonl\"\n",
    "    if not events_file.exists():\n",
    "        print(f\"Warning: {events_file} not found for job {job_id}\")\n",
    "        continue\n",
    "    \n",
    "    with open(events_file) as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        if not lines:\n",
    "            print(f\"Warning: {events_file} is empty for job {job_id}\")\n",
    "            continue\n",
    "        last_event = json.loads(lines[-1])\n",
    "    \n",
    "    # Read override.yaml\n",
    "    override_file = run_dir / \".hydra\" / \"overrides.yaml\"\n",
    "    overrides = {}\n",
    "    if override_file.exists():\n",
    "        with open(override_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('- '):\n",
    "                    line = line[2:]  # Remove '- '\n",
    "                if '=' in line:\n",
    "                    key, value = line.split('=', 1)\n",
    "                    overrides[key.strip()] = value.strip()\n",
    "    \n",
    "    # Extract hyperparameters\n",
    "    positional = overrides.get('classifier.model.positional', 'unknown')\n",
    "    model_size = overrides.get('+classifier/model_size', 'unknown')\n",
    "    \n",
    "    # Extract histories from last event - check both histories dict and direct fields\n",
    "    histories = last_event.get('histories', {})\n",
    "    history_train_loss = last_event.get('history_train_loss', histories.get('train_loss', []))\n",
    "    history_val_loss = last_event.get('history_val_loss', histories.get('val_loss', []))\n",
    "    history_val_auroc = last_event.get('history_val_auroc', histories.get('val_auroc', []))\n",
    "    \n",
    "    runs_data.append({\n",
    "        'job_id': job_id,\n",
    "        'run_dir': str(run_dir),\n",
    "        'positional': positional,\n",
    "        'model_size': model_size,\n",
    "        'history_train_loss': history_train_loss,\n",
    "        'history_val_loss': history_val_loss,\n",
    "        'history_val_auroc': history_val_auroc,\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(runs_data)} runs\")\n",
    "df = pd.DataFrame(runs_data)\n",
    "print(f\"/nPositional encodings found: {sorted(df['positional'].unique())}\")\n",
    "print(f\"Model sizes found: {sorted(df['model_size'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae093b9",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c005b6c",
   "metadata": {},
   "source": [
    "### 1.1.1 Validation Loss by Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by positional encoding\n",
    "positional_values = sorted(df['positional'].unique())\n",
    "# Use highly distinct colors\n",
    "distinct_colors = ['#1f77b4', '#d62728', '#2ca02c', '#ff7f0e', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "colors = [distinct_colors[i % len(distinct_colors)] for i in range(len(positional_values))]\n",
    "\n",
    "# Val Loss\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, pos_val in enumerate(positional_values):\n",
    "    subset = df[df['positional'] == pos_val]\n",
    "    for _, row in subset.iterrows():\n",
    "        epochs = range(1, len(row['history_val_loss']) + 1)\n",
    "        ax.plot(epochs, row['history_val_loss'], \n",
    "                color=colors[i], alpha=0.15, linewidth=1)\n",
    "    # Plot mean\n",
    "    all_val_losses = []\n",
    "    max_len = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        all_val_losses.append(row['history_val_loss'])\n",
    "        max_len = max(max_len, len(row['history_val_loss']))\n",
    "    if all_val_losses:\n",
    "        padded = [loss + [loss[-1]] * (max_len - len(loss)) if len(loss) < max_len else loss \n",
    "                 for loss in all_val_losses]\n",
    "        mean_loss = np.mean([loss[:max_len] for loss in padded], axis=0)\n",
    "        epochs = range(1, len(mean_loss) + 1)\n",
    "        ax.plot(epochs, mean_loss, color=colors[i], linewidth=2, \n",
    "                label=f'{pos_val} (n={len(subset)})')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Val Loss', fontsize=14)\n",
    "ax.set_title('Validation Loss by Positional Encoding', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28e5c7",
   "metadata": {},
   "source": [
    "### 1.1.2 Val AUROC by Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val AUROC\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, pos_val in enumerate(positional_values):\n",
    "    subset = df[df['positional'] == pos_val]\n",
    "    for _, row in subset.iterrows():\n",
    "        if row['history_val_auroc']:\n",
    "            epochs = range(1, len(row['history_val_auroc']) + 1)\n",
    "            ax.plot(epochs, row['history_val_auroc'], \n",
    "                    color=colors[i], alpha=0.15, linewidth=1)\n",
    "    # Plot mean\n",
    "    all_val_aurocs = []\n",
    "    max_len = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        if row['history_val_auroc']:\n",
    "            all_val_aurocs.append(row['history_val_auroc'])\n",
    "            max_len = max(max_len, len(row['history_val_auroc']))\n",
    "    if all_val_aurocs:\n",
    "        padded = [auroc + [auroc[-1]] * (max_len - len(auroc)) if len(auroc) < max_len else auroc \n",
    "                 for auroc in all_val_aurocs]\n",
    "        mean_auroc = np.mean([auroc[:max_len] for auroc in padded], axis=0)\n",
    "        epochs = range(1, len(mean_auroc) + 1)\n",
    "        ax.plot(epochs, mean_auroc, color=colors[i], linewidth=2, \n",
    "                label=f'{pos_val} (n={len(subset)})')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Val AUROC', fontsize=14)\n",
    "ax.set_title('Val AUROC by Positional Encoding', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83aa50",
   "metadata": {},
   "source": [
    "### 1.1.3 ROC Curves Grouped by Positional Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and run inference for ROC curves\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from thesis_ml.reports.inference.classification import run_classification_inference\n",
    "from thesis_ml.utils.paths import get_run_id\n",
    "\n",
    "# Hardcoded paths\n",
    "base_dir = Path(\"C:/Users/niels/Projects/Thesis-Code/Saved models/20251126-150519_compare_positional_encodings\")\n",
    "run_prefix = \"run_20251126-150519_compare_positional_encodings\"\n",
    "data_path = Path(\"C:/Users/niels/Projects/Thesis-Code/Data/4tops_splitted.h5\")\n",
    "\n",
    "print(f\"Loading {len(df)} models for inference...\")\n",
    "models = []\n",
    "for _, row in df.iterrows():\n",
    "    run_dir = Path(row['run_dir'])\n",
    "    run_id = get_run_id(run_dir)\n",
    "    \n",
    "    try:\n",
    "        # Load config from .hydra/config.yaml\n",
    "        cfg_path = run_dir / \".hydra\" / \"config.yaml\"\n",
    "        cfg = OmegaConf.load(str(cfg_path))\n",
    "        \n",
    "        # Override data path to use local file\n",
    "        cfg.data.path = str(data_path)\n",
    "        \n",
    "        # Load model weights\n",
    "        weights_path = run_dir / \"best_val.pt\"\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Build classifier model\n",
    "        from thesis_ml.architectures.transformer_classifier.base import build_from_config as build_classifier\n",
    "        from thesis_ml.data.h5_loader import make_classification_dataloaders\n",
    "        from thesis_ml.training_loops.transformer_classifier import _gather_meta\n",
    "        \n",
    "        # Populate meta if missing\n",
    "        if not hasattr(cfg, \"meta\") or cfg.meta is None:\n",
    "            train_dl, val_dl, test_dl, meta = make_classification_dataloaders(cfg)\n",
    "            _gather_meta(cfg, meta)\n",
    "        \n",
    "        model = build_classifier(cfg, cfg.meta).to(device)\n",
    "        \n",
    "        # Load weights\n",
    "        checkpoint = torch.load(str(weights_path), map_location=device, weights_only=False)\n",
    "        state_dict = checkpoint[\"model_state_dict\"] if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint else checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        models.append((run_id, cfg, model))\n",
    "        print(f\"  Loaded {run_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load {run_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully loaded {len(models)} models\")\n",
    "\n",
    "# Use first model's config as base\n",
    "base_cfg = models[0][1] if models else None\n",
    "if base_cfg is None:\n",
    "    print(\"Warning: No models loaded - cannot run inference\")\n",
    "    inference_results = {}\n",
    "else:\n",
    "    # Run classification inference\n",
    "    inference_results = run_classification_inference(\n",
    "        models=models,\n",
    "        dataset_cfg=base_cfg,\n",
    "        split='val',  # Use validation set\n",
    "        inference_cfg={\n",
    "            \"autocast\": False,\n",
    "            \"batch_size\": 512,\n",
    "            \"seed\": 42,\n",
    "            \"max_samples\": None,\n",
    "        },\n",
    "    )\n",
    "    print(f\"Completed inference for {len(inference_results)} models\")\n",
    "\n",
    "# Create ROC curves grouped by positional encoding\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for i, pos_val in enumerate(positional_values):\n",
    "    subset = df[df['positional'] == pos_val]\n",
    "    \n",
    "    # Collect ROC curves for all runs with this positional encoding\n",
    "    for _, row in subset.iterrows():\n",
    "        run_id = get_run_id(Path(row['run_dir']))\n",
    "        if run_id in inference_results:\n",
    "            metrics = inference_results[run_id]\n",
    "            roc_curves_data = metrics.get(\"roc_curves\", {})\n",
    "            \n",
    "            # For binary classification, get the single ROC curve\n",
    "            if roc_curves_data and len(roc_curves_data) == 1:\n",
    "                class_idx = list(roc_curves_data.keys())[0]\n",
    "                curve = roc_curves_data[class_idx]\n",
    "                fpr, tpr = curve['fpr'], curve['tpr']\n",
    "                ax.plot(fpr, tpr, color=colors[i], alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Plot mean ROC curve for this positional encoding\n",
    "    all_fprs = []\n",
    "    all_tprs = []\n",
    "    for _, row in subset.iterrows():\n",
    "        run_id = get_run_id(Path(row['run_dir']))\n",
    "        if run_id in inference_results:\n",
    "            metrics = inference_results[run_id]\n",
    "            roc_curves_data = metrics.get(\"roc_curves\", {})\n",
    "            if roc_curves_data and len(roc_curves_data) == 1:\n",
    "                class_idx = list(roc_curves_data.keys())[0]\n",
    "                curve = roc_curves_data[class_idx]\n",
    "                all_fprs.append(curve['fpr'])\n",
    "                all_tprs.append(curve['tpr'])\n",
    "    \n",
    "    if all_fprs:\n",
    "        # Interpolate to common FPR points for averaging\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        mean_tprs = []\n",
    "        for fpr, tpr in zip(all_fprs, all_tprs, strict=False):\n",
    "            mean_tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        mean_tpr = np.mean(mean_tprs, axis=0)\n",
    "        \n",
    "        # Calculate mean AUROC\n",
    "        mean_auroc = np.mean([metrics.get(\"auroc\", 0) for run_id in \n",
    "                             [get_run_id(Path(r)) for r in subset['run_dir']] \n",
    "                             if run_id in inference_results])\n",
    "        \n",
    "        ax.plot(mean_fpr, mean_tpr, color=colors[i], linewidth=2,\n",
    "                label=f'{pos_val} (mean AUROC={mean_auroc:.3f}, n={len(subset)})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "ax.set_title('ROC Curves by Positional Encoding', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e4e0b",
   "metadata": {},
   "source": [
    "## 2. Model Size Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381eaf1c",
   "metadata": {},
   "source": [
    "### 2.1.1 Validation Loss by Model Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model_size\n",
    "model_size_values = sorted(df['model_size'].unique())\n",
    "# Use highly distinct colors\n",
    "colors_size = [distinct_colors[i % len(distinct_colors)] for i in range(len(model_size_values))]\n",
    "\n",
    "# Val Loss\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, size_val in enumerate(model_size_values):\n",
    "    subset = df[df['model_size'] == size_val]\n",
    "    for _, row in subset.iterrows():\n",
    "        epochs = range(1, len(row['history_val_loss']) + 1)\n",
    "        ax.plot(epochs, row['history_val_loss'], \n",
    "                color=colors_size[i], alpha=0.15, linewidth=1)\n",
    "    # Plot mean\n",
    "    all_val_losses = []\n",
    "    max_len = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        all_val_losses.append(row['history_val_loss'])\n",
    "        max_len = max(max_len, len(row['history_val_loss']))\n",
    "    if all_val_losses:\n",
    "        padded = [loss + [loss[-1]] * (max_len - len(loss)) if len(loss) < max_len else loss \n",
    "                 for loss in all_val_losses]\n",
    "        mean_loss = np.mean([loss[:max_len] for loss in padded], axis=0)\n",
    "        epochs = range(1, len(mean_loss) + 1)\n",
    "        ax.plot(epochs, mean_loss, color=colors_size[i], linewidth=2, \n",
    "                label=f'{size_val} (n={len(subset)})')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Val Loss', fontsize=14)\n",
    "ax.set_title('Validation Loss by Model Size', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102d60c",
   "metadata": {},
   "source": [
    "### 2.1.2 Val AUROC by Model Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val AUROC\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, size_val in enumerate(model_size_values):\n",
    "    subset = df[df['model_size'] == size_val]\n",
    "    for _, row in subset.iterrows():\n",
    "        if row['history_val_auroc']:\n",
    "            epochs = range(1, len(row['history_val_auroc']) + 1)\n",
    "            ax.plot(epochs, row['history_val_auroc'], \n",
    "                    color=colors_size[i], alpha=0.15, linewidth=1)\n",
    "    # Plot mean\n",
    "    all_val_aurocs = []\n",
    "    max_len = 0\n",
    "    for _, row in subset.iterrows():\n",
    "        if row['history_val_auroc']:\n",
    "            all_val_aurocs.append(row['history_val_auroc'])\n",
    "            max_len = max(max_len, len(row['history_val_auroc']))\n",
    "    if all_val_aurocs:\n",
    "        padded = [auroc + [auroc[-1]] * (max_len - len(auroc)) if len(auroc) < max_len else auroc \n",
    "                 for auroc in all_val_aurocs]\n",
    "        mean_auroc = np.mean([auroc[:max_len] for auroc in padded], axis=0)\n",
    "        epochs = range(1, len(mean_auroc) + 1)\n",
    "        ax.plot(epochs, mean_auroc, color=colors_size[i], linewidth=2, \n",
    "                label=f'{size_val} (n={len(subset)})')\n",
    "ax.set_xlabel('Epoch', fontsize=14)\n",
    "ax.set_ylabel('Val AUROC', fontsize=14)\n",
    "ax.set_title('Val AUROC by Model Size', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf58ad4",
   "metadata": {},
   "source": [
    "### 2.1.3 ROC Curves by Model Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446859c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curves grouped by model size\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for i, size_val in enumerate(model_size_values):\n",
    "    subset = df[df['model_size'] == size_val]\n",
    "    \n",
    "    # Collect ROC curves for all runs with this model size\n",
    "    for _, row in subset.iterrows():\n",
    "        run_id = get_run_id(Path(row['run_dir']))\n",
    "        if run_id in inference_results:\n",
    "            metrics = inference_results[run_id]\n",
    "            roc_curves_data = metrics.get(\"roc_curves\", {})\n",
    "            \n",
    "            # For binary classification, get the single ROC curve\n",
    "            if roc_curves_data and len(roc_curves_data) == 1:\n",
    "                class_idx = list(roc_curves_data.keys())[0]\n",
    "                curve = roc_curves_data[class_idx]\n",
    "                fpr, tpr = curve['fpr'], curve['tpr']\n",
    "                ax.plot(fpr, tpr, color=colors_size[i], alpha=0.3, linewidth=1)\n",
    "    \n",
    "    # Plot mean ROC curve for this model size\n",
    "    all_fprs = []\n",
    "    all_tprs = []\n",
    "    for _, row in subset.iterrows():\n",
    "        run_id = get_run_id(Path(row['run_dir']))\n",
    "        if run_id in inference_results:\n",
    "            metrics = inference_results[run_id]\n",
    "            roc_curves_data = metrics.get(\"roc_curves\", {})\n",
    "            if roc_curves_data and len(roc_curves_data) == 1:\n",
    "                class_idx = list(roc_curves_data.keys())[0]\n",
    "                curve = roc_curves_data[class_idx]\n",
    "                all_fprs.append(curve['fpr'])\n",
    "                all_tprs.append(curve['tpr'])\n",
    "    \n",
    "    if all_fprs:\n",
    "        # Interpolate to common FPR points for averaging\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        mean_tprs = []\n",
    "        for fpr, tpr in zip(all_fprs, all_tprs, strict=False):\n",
    "            mean_tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        mean_tpr = np.mean(mean_tprs, axis=0)\n",
    "        \n",
    "        # Calculate mean AUROC\n",
    "        mean_auroc = np.mean([metrics.get(\"auroc\", 0) for run_id in \n",
    "                             [get_run_id(Path(r)) for r in subset['run_dir']] \n",
    "                             if run_id in inference_results])\n",
    "        \n",
    "        ax.plot(mean_fpr, mean_tpr, color=colors_size[i], linewidth=2,\n",
    "                label=f'{size_val} (mean AUROC={mean_auroc:.3f}, n={len(subset)})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=14)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=14)\n",
    "ax.set_title('ROC Curves by Model Size', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226b9a2",
   "metadata": {},
   "source": [
    "## 3. Score Distributions for All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create score distribution plots for all models\n",
    "\n",
    "# Determine grid size (e.g., 3x4 for 12 models, or adjust for 11)\n",
    "num_models = len(df)\n",
    "n_cols = 4\n",
    "n_rows = (num_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten() if num_models > 1 else [axes]\n",
    "\n",
    "for idx, (_, row) in enumerate(df.iterrows()):\n",
    "    run_id = get_run_id(Path(row['run_dir']))\n",
    "    \n",
    "    if run_id not in inference_results:\n",
    "        axes[idx].text(0.5, 0.5, f'No data\\\\n{run_id}', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].set_title(f\"Job {row['job_id']}: {row['positional']} / {row['model_size']}\", \n",
    "                           fontsize=10)\n",
    "        continue\n",
    "    \n",
    "    metrics = inference_results[run_id]\n",
    "    \n",
    "    # Extract per-event scores and labels\n",
    "    per_event_scores = metrics.get(\"per_event_scores\")\n",
    "    per_event_labels = metrics.get(\"per_event_labels\")\n",
    "    \n",
    "    if per_event_scores is None or per_event_labels is None:\n",
    "        axes[idx].text(0.5, 0.5, 'No score data', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].set_title(f\"Job {row['job_id']}: {row['positional']} / {row['model_size']}\", \n",
    "                           fontsize=10)\n",
    "        continue\n",
    "    \n",
    "    scores = np.array(per_event_scores)\n",
    "    labels = np.array(per_event_labels)\n",
    "    \n",
    "    # Split into signal and background (assuming binary classification)\n",
    "    signal_class_idx = 1\n",
    "    background_class_idx = 0\n",
    "    signal_scores = scores[labels == signal_class_idx]\n",
    "    background_scores = scores[labels == background_class_idx]\n",
    "    \n",
    "    if len(signal_scores) == 0 or len(background_scores) == 0:\n",
    "        axes[idx].text(0.5, 0.5, 'Insufficient data', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].set_title(f\"Job {row['job_id']}: {row['positional']} / {row['model_size']}\", \n",
    "                           fontsize=10)\n",
    "        continue\n",
    "    \n",
    "    # Compute optimal threshold using Youden's J\n",
    "    y_binary = (labels == signal_class_idx).astype(int)\n",
    "    fpr, tpr, thresholds = roc_curve(y_binary, scores)\n",
    "    youden_j = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_j)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    # Compute overlap area\n",
    "    score_min = min(scores.min(), background_scores.min(), signal_scores.min())\n",
    "    score_max = max(scores.max(), background_scores.max(), signal_scores.max())\n",
    "    bins = np.linspace(score_min, score_max, 50)\n",
    "    \n",
    "    # Compute normalized histograms\n",
    "    hist_bg, _ = np.histogram(background_scores, bins=bins, density=True)\n",
    "    hist_sig, _ = np.histogram(signal_scores, bins=bins, density=True)\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = np.minimum(hist_bg, hist_sig)\n",
    "    overlap_area = np.trapz(overlap, bins[:-1])\n",
    "    overlap_pct = overlap_area * 100\n",
    "    \n",
    "    # Plot histograms\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    axes[idx].hist(background_scores, bins=bins, alpha=0.5, label='Background', \n",
    "                  density=True, color='blue')\n",
    "    axes[idx].hist(signal_scores, bins=bins, alpha=0.5, label='Signal', \n",
    "                  density=True, color='red')\n",
    "    axes[idx].axvline(optimal_threshold, color='green', linestyle='--', \n",
    "                     linewidth=2, label='Optimal threshold')\n",
    "    \n",
    "    # Get AUROC\n",
    "    auroc = metrics.get(\"auroc\", 0.0)\n",
    "    \n",
    "    axes[idx].set_xlabel('Score', fontsize=9)\n",
    "    axes[idx].set_ylabel('Density', fontsize=9)\n",
    "    axes[idx].set_title(f\"Job {row['job_id']}: {row['positional']} / {row['model_size']}\\\\n\"\n",
    "                       f\"Overlap: {overlap_pct:.1f}%, AUROC: {auroc:.3f}\", \n",
    "                       fontsize=9)\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa2648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9454c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3c6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
