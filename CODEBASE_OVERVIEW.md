# Codebase Overview

**Complete visual walkthrough of the thesis-ml codebase structure and workflows.**

## ðŸ“Š System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          THESIS-ML SYSTEM                             â”‚
â”‚                                                                       â”‚
â”‚  Entry Points:  thesis-train    thesis-report    python notebook    â”‚
â”‚                      â”‚               â”‚                 â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚               â”‚                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                 â”‚
          â”‚                     â”‚      â”‚                 â”‚
          â–¼                     â–¼      â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CLI/    â”‚          â”‚         CORE MODULES             â”‚
    â”‚ train   â”‚          â”‚                                  â”‚
    â”‚         â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚DISPATCH â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â–ºâ”‚ Training   â”‚  â”‚Architec-   â”‚ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚ Loops      â”‚  â”‚tures       â”‚ â”‚
                         â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚         â”‚                        â”‚
    â”‚ CLI/    â”‚          â”‚         â”‚                        â”‚
    â”‚ reports â”‚          â”‚         â–¼                        â”‚
    â”‚         â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚ analyze â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â–ºâ”‚   Facts    â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚ Writers    â”‚                 â”‚
                         â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚
                         â”‚         â”‚                        â”‚
                         â”‚         â–¼                        â”‚
                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                         â”‚  â”‚Monitoring  â”‚  â”‚   Data     â”‚ â”‚
                         â”‚  â”‚ (plots)    â”‚  â”‚  Loaders   â”‚ â”‚
                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   File System    â”‚
                         â”‚                  â”‚
                         â”‚  outputs/        â”‚
                         â”‚    â”œâ”€ runs/      â”‚
                         â”‚    â”‚   â”œâ”€facts/  â”‚
                         â”‚    â”‚   â””â”€figures/â”‚
                         â”‚    â””â”€ reports/   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–²
                                    â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Facts Readers  â”‚
                         â”‚                  â”‚
                         â”‚   load_runs()    â”‚
                         â”‚   discover()     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ Directory Tree

```
thesis-ml/
â”‚
â”œâ”€â”€ configs/                    # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml             # Root composition
â”‚   â”œâ”€â”€ data/                   # Dataset configs
â”‚   â”‚   â”œâ”€â”€ h5_tokens.yaml
â”‚   â”‚   â””â”€â”€ synthetic.yaml
â”‚   â”œâ”€â”€ phase1/                 # Autoencoder configs
â”‚   â”‚   â”œâ”€â”€ encoder/            # Encoder architectures
â”‚   â”‚   â”œâ”€â”€ decoder/            # Decoder architectures
â”‚   â”‚   â”œâ”€â”€ latent_space/       # Bottleneck types
â”‚   â”‚   â”œâ”€â”€ trainer/            # Training hyperparameters
â”‚   â”‚   â””â”€â”€ experiment/         # Pre-defined experiments
â”‚   â”œâ”€â”€ logging/                # Monitoring/plot configs
â”‚   â”œâ”€â”€ env/                    # Environment (local/stoomboot)
â”‚   â””â”€â”€ report/                 # Report configs
â”‚
â”œâ”€â”€ src/thesis_ml/              # Main package
â”‚   â”‚
â”‚   â”œâ”€â”€ cli/                    # Command-line interface
â”‚   â”‚   â”œâ”€â”€ train/              # thesis-train entry point
â”‚   â”‚   â”‚   â”œâ”€â”€ __main__.py     # Hydra CLI
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py     # DISPATCH registry
â”‚   â”‚   â””â”€â”€ reports/            # thesis-report entry point
â”‚   â”‚       â””â”€â”€ __main__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training_loops/         # Training implementations
â”‚   â”‚   â”œâ”€â”€ autoencoder.py      # Standard AE
â”‚   â”‚   â”œâ”€â”€ gan_autoencoder.py  # GAN AE
â”‚   â”‚   â”œâ”€â”€ diffusion_autoencoder.py
â”‚   â”‚   â””â”€â”€ simple_mlp.py       # Test loop
â”‚   â”‚
â”‚   â”œâ”€â”€ architectures/          # Model definitions
â”‚   â”‚   â”œâ”€â”€ autoencoder/
â”‚   â”‚   â”‚   â”œâ”€â”€ assembly.py     # build_from_config()
â”‚   â”‚   â”‚   â”œâ”€â”€ encoders/       # MLP, GNN, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ decoders/       # MLP, GNN, etc.
â”‚   â”‚   â”‚   â”œâ”€â”€ bottlenecks/    # VQ, linear, identity
â”‚   â”‚   â”‚   â””â”€â”€ losses/         # Reconstruction, adversarial
â”‚   â”‚   â””â”€â”€ simple/
â”‚   â”‚       â””â”€â”€ mlp.py
â”‚   â”‚
â”‚   â”œâ”€â”€ facts/                  # Facts system (NEW!)
â”‚   â”‚   â”œâ”€â”€ builders.py         # build_event_payload()
â”‚   â”‚   â”œâ”€â”€ writers.py          # append_jsonl_event(), append_scalars_csv()
â”‚   â”‚   â””â”€â”€ readers.py          # load_runs(), discover_runs()
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/             # Training-time visualization (was plots/)
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # handle_event()
â”‚   â”‚   â”œâ”€â”€ io_utils.py         # save_figure()
â”‚   â”‚   â””â”€â”€ families/           # losses, metrics, recon, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                # Post-training analysis
â”‚   â”‚   â”œâ”€â”€ analyses/           # Report implementations (was experiments/)
â”‚   â”‚   â”‚   â”œâ”€â”€ compare_tokenizers.py
â”‚   â”‚   â”‚   â””â”€â”€ compare_globals_heads.py
â”‚   â”‚   â”œâ”€â”€ inference/          # Inference utilities
â”‚   â”‚   â”œâ”€â”€ plots/              # Report plotting
â”‚   â”‚   â””â”€â”€ utils/              # IO, manifest, backlinks
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                   # Dataset loaders
â”‚   â”‚   â”œâ”€â”€ h5_loader.py
â”‚   â”‚   â””â”€â”€ synthetic.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # General utilities
â”‚       â”œâ”€â”€ seed.py
â”‚       â”œâ”€â”€ paths.py
â”‚       â””â”€â”€ training_progress_shower.py
â”‚
â”œâ”€â”€ outputs/                    # All training/report outputs
â”‚   â”œâ”€â”€ runs/                   # Single runs
â”‚   â”‚   â””â”€â”€ run_YYYYMMDD-HHMMSS_name/
â”‚   â”‚       â”œâ”€â”€ .hydra/
â”‚   â”‚       â”œâ”€â”€ facts/
â”‚   â”‚       â”œâ”€â”€ figures/
â”‚   â”‚       â””â”€â”€ *.pt
â”‚   â”œâ”€â”€ multiruns/              # Sweeps
â”‚   â”‚   â””â”€â”€ exp_YYYYMMDD-HHMMSS_name/
â”‚   â””â”€â”€ reports/                # Generated reports
â”‚       â””â”€â”€ report_YYYYMMDD-HHMMSS_name/
â”‚
â”œâ”€â”€ hpc/stoomboot/              # HPC submission scripts
â”‚   â”œâ”€â”€ train.sh
â”‚   â”œâ”€â”€ train.sub
â”‚   â”œâ”€â”€ report.sh
â”‚   â””â”€â”€ report.sub
â”‚
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ scripts/                    # Utility scripts
â””â”€â”€ notebooks/                  # Jupyter notebooks
```

## ðŸ”„ Key Workflows

### Workflow 1: Train Locally

```
1. User: thesis-train phase1.trainer.epochs=20 phase1/latent_space=vq

2. CLI: Parse config via Hydra
   â””â”€> configs/config.yaml + overrides

3. Dispatch: Select training loop
   â””â”€> DISPATCH["ae"] â†’ training_loops/autoencoder.py

4. Training Loop:
   â”œâ”€ Load data (data/h5_loader.py)
   â”œâ”€ Build model (architectures/autoencoder/assembly.py)
   â”‚   â”œâ”€ Encoder (architectures/autoencoder/encoders/mlp.py)
   â”‚   â”œâ”€ Bottleneck (architectures/autoencoder/bottlenecks/vq.py)
   â”‚   â””â”€ Decoder (architectures/autoencoder/decoders/mlp.py)
   â”œâ”€ Training epochs
   â”‚   â””â”€ Each epoch:
   â”‚       â”œâ”€ Emit facts (facts/writers.py)
   â”‚       â”‚   â”œâ”€> facts/events.jsonl
   â”‚       â”‚   â””â”€> facts/scalars.csv
   â”‚       â””â”€ Create plots (monitoring/orchestrator.py)
   â”‚           â””â”€> figures/*.png
   â””â”€ Save checkpoints
       â””â”€> best_val.pt, last.pt

5. Output: outputs/runs/run_YYYYMMDD-HHMMSS_/
```

### Workflow 2: Multi-Run Sweep

```
1. User: thesis-train --multirun phase1/latent_space=none,vq,linear

2. Hydra: Create 3 jobs (job0, job1, job2)

3. For each job:
   â””â”€> Run Workflow 1

4. Output:
   â”œâ”€> outputs/multiruns/exp_YYYYMMDD-HHMMSS_experiment/
   â””â”€> outputs/runs/
       â”œâ”€ run_YYYYMMDD-HHMMSS_experiment_job0/  # latent_space=none
       â”œâ”€ run_YYYYMMDD-HHMMSS_experiment_job1/  # latent_space=vq
       â””â”€ run_YYYYMMDD-HHMMSS_experiment_job2/  # latent_space=linear
```

### Workflow 3: Generate Report

```
1. User: thesis-report --config-name compare_tokenizers \
          inputs.sweep_dir=outputs/multiruns/exp_*/

2. CLI: Load report config
   â””â”€> configs/report/compare_tokenizers.yaml

3. Report Execution (reports/analyses/compare_tokenizers.py):
   â”œâ”€ Discover runs
   â”‚   â””â”€> facts/readers.py::discover_runs()
   â”‚       â””â”€> Find all runs matching sweep_dir
   â”‚
   â”œâ”€ Load facts from all runs
   â”‚   â””â”€> facts/readers.py::load_runs()
   â”‚       â”œâ”€ Read .hydra/config.yaml
   â”‚       â”œâ”€ Read facts/events.jsonl
   â”‚       â”œâ”€ Read facts/scalars.csv
   â”‚       â””â”€ Return (runs_df, per_epoch, order)
   â”‚
   â”œâ”€ Aggregate & analyze
   â”‚   â”œâ”€ Group by tokenizer
   â”‚   â”œâ”€ Compute statistics
   â”‚   â””â”€ Filter/sort runs
   â”‚
   â”œâ”€ Generate training plots
   â”‚   â””â”€> reports/plots/*.py
   â”‚       â””â”€> training/figures/*.png
   â”‚
   â”œâ”€ (Optional) Run inference
   â”‚   â””â”€> reports/inference/*.py
   â”‚       â””â”€> inference/figures/*.png
   â”‚
   â””â”€ Save summaries
       â”œâ”€> training/summary.csv
       â”œâ”€> training/summary.json
       â””â”€> manifest.yaml

4. Output: outputs/reports/report_YYYYMMDD-HHMMSS_compare_tokenizers/
```

### Workflow 4: HPC Submission

```
1. User (local): Edit hpc/stoomboot/train.sub
   â””â”€> Set arguments: phase1.trainer.epochs=50 ...

2. User (local): git push

3. SSH to Stoomboot: ssh stoomboot

4. User (HPC):
   â”œâ”€ cd /project/atlas/users/nterlind/Thesis-Code
   â”œâ”€ git pull
   â””â”€ condor_submit hpc/stoomboot/train.sub

5. Condor: Submit job to cluster

6. Job execution:
   â”œâ”€ Activate conda env
   â”œâ”€ Run: thesis-train env=stoomboot ...
   â””â”€> (Workflow 1 with HPC paths)

7. Output: /data/atlas/users/nterlind/outputs/runs/...

8. Monitor: condor_q
```

## ðŸ§© Component Interaction Map

### Training Phase

```
User Command
    â”‚
    â–¼
cli/train/__main__.py
    â”‚
    â”œâ”€ Load config (Hydra)
    â”œâ”€ Validate (legacy key check)
    â””â”€ Dispatch to loop
         â”‚
         â–¼
training_loops/autoencoder.py
         â”‚
         â”œâ”€ data/h5_loader â†’ DataLoaders
         â”œâ”€ architectures/autoencoder/assembly â†’ Model
         â”œâ”€ torch.optim â†’ Optimizer
         â”‚
         â”œâ”€ FOR each epoch:
         â”‚   â”œâ”€ Forward/backward pass
         â”‚   â”œâ”€ facts/writers â†’ events.jsonl, scalars.csv
         â”‚   â””â”€ monitoring/orchestrator â†’ figures/*.png
         â”‚
         â””â”€ Save checkpoints â†’ *.pt
```

### Reporting Phase

```
User Command
    â”‚
    â–¼
cli/reports/__main__.py
    â”‚
    â”œâ”€ Load config (Hydra)
    â””â”€ Dispatch to report
         â”‚
         â–¼
reports/analyses/compare_tokenizers.py
         â”‚
         â”œâ”€ facts/readers â†’ (runs_df, per_epoch)
         â”œâ”€ Aggregate/filter runs
         â”œâ”€ reports/plots â†’ training/figures/*.png
         â”œâ”€ reports/inference (optional) â†’ inference/figures/*.png
         â””â”€ Save summaries â†’ *.csv, *.json
```

## ðŸ“‹ Key Files Reference

### Entry Points

| File | Purpose | Command |
|------|---------|---------|
| `cli/train/__main__.py` | Training CLI | `thesis-train` |
| `cli/reports/__main__.py` | Reports CLI | `thesis-report` |

### Core Logic

| File | Purpose |
|------|---------|
| `training_loops/autoencoder.py` | Standard AE training |
| `architectures/autoencoder/assembly.py` | Build model from config |
| `facts/builders.py` | Create event payloads |
| `facts/writers.py` | Write facts to disk |
| `facts/readers.py` | Read facts from runs |
| `monitoring/orchestrator.py` | Route events to plot families |
| `reports/analyses/compare_tokenizers.py` | Compare VQ vs non-VQ |

### Configuration

| File | Purpose |
|------|---------|
| `configs/config.yaml` | Root composition |
| `configs/phase1/encoder/*.yaml` | Encoder configs |
| `configs/phase1/latent_space/*.yaml` | Bottleneck configs |
| `configs/phase1/trainer/ae.yaml` | Training hyperparameters |
| `configs/report/compare_tokenizers.yaml` | Report config |

## ðŸŽ¯ Design Highlights

### 1. Facts-First Architecture
- **Training emits** â†’ Facts written to disk
- **Reports consume** â†’ Facts read from disk
- **Benefit**: Post-hoc analysis without re-training

### 2. Hydra-Driven Configuration
- **All parameters** via config files
- **CLI overrides** for flexibility
- **Reproducibility** via `.hydra/config.yaml` snapshot

### 3. Modular Components
- **Training loops**: Independent implementations
- **Architectures**: Composable encoders/decoders/bottlenecks
- **Reports**: Reusable analysis patterns

### 4. Environment Agnostic
- **Same code**, different configs
- **Local**: `env=local` (Windows paths)
- **HPC**: `env=stoomboot` (Linux paths)

## ðŸš€ Quick Reference Commands

```bash
# Training
thesis-train                                      # Default config
thesis-train phase1.trainer.epochs=20             # Override epochs
thesis-train phase1/latent_space=vq               # Use VQ bottleneck
thesis-train --multirun phase1/latent_space=none,vq  # Sweep

# Reports
thesis-report --config-name compare_tokenizers inputs.sweep_dir=outputs/multiruns/exp_*

# HPC
condor_submit hpc/stoomboot/train.sub
condor_q

# Development
pip install -e .
pytest
```

## ðŸ“š Documentation Index

1. **[README.md](README.md)**: Project overview and quick start
2. **[ARCHITECTURE.md](ARCHITECTURE.md)**: Detailed system design
3. **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: Using/creating training code
4. **[REPORTS_GUIDE.md](REPORTS_GUIDE.md)**: Using/creating reports
5. **[FACTS_SYSTEM.md](FACTS_SYSTEM.md)**: Facts architecture
6. **[CONFIGS_GUIDE.md](CONFIGS_GUIDE.md)**: Hydra patterns (to be created)
7. **[HPC_GUIDE.md](HPC_GUIDE.md)**: Stoomboot usage (to be created)
8. **[DEVELOPMENT_GUIDE.md](DEVELOPMENT_GUIDE.md)**: Development setup (to be created)

This document provides the 10,000-foot view. Dive into specific guides for details.
